%------------------------------------------------------------------------------
% Template file for the submission of articles to IUCr journals in LaTeX2e
% using the iucrjournals document class (file iucrjournals.cls)
% This work has been dedicated to the public domain
% License: CC0 1.0 Universal
% https://creativecommons.org/publicdomain/zero/1.0/
%------------------------------------------------------------------------------
% This template file and associated class and style files produce documents in
% a preprint style suitable for submission and review purposes.
% The iucrjournals.cls requires a small selection of packages from standard TeXLive
% distributions and contains a minimal set of macros to define content and apply
% formatting. BibTeX and iucr.bst should be used for references (using harvard.sty).
% If you wish to use additional packages, please reference them in this document and
% please only use packages included in standard TeXLive distributions in order to
% avoid compilation problems during the submission process.
%------------------------------------------------------------------------------

\documentclass{iucrjournals}

% Add extra packages here, e.g.
% \usepackage{myfavouritepackage}
\usepackage{amssymb}
\usepackage[fleqn]{amsmath}

\usepackage{booktabs}
\usepackage[table]{xcolor}
%\usepackage[table]{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{caption}

\usepackage[section]{placeins}


\DeclareMathOperator{\sinc}{sinc}
\newcommand{\order}{\mathcal{O}}


%internal refs with macros, for consistency:
\newcommand{\labsec}[1]{\label{sec:#1}}
\newcommand{\refsecnumonly}[1]{\ref{sec:#1}}
\newcommand{\refsec}[1]{Section~\refsecnumonly{#1}}
\newcommand{\Refsec}[1]{Section~\refsecnumonly{#1}}%same as \refsec, but always capitalised!
\newcommand{\reftwosections}[2]{Sections~\refsecnumonly{#1} and \refsecnumonly{#2}}
\newcommand{\refthreesections}[3]{Sections~\refsecnumonly{#1}, \refsecnumonly{#2}, and \refsecnumonly{#3}}
\newcommand{\refsectionrange}[2]{Sections~\refsecnumonly{#1}--\refsecnumonly{#2}}

\newcommand{\labappendix}[1]{\label{app:#1}}
\newcommand{\refappendixnumonly}[1]{\ref{app:#1}}
\newcommand{\refappendix}[1]{\refappendixnumonly{#1}}

\newcommand{\refalgnumonly}[1]{\ref{alg:#1}}
\newcommand{\refalg}[1]{Algorithm~\refalgnumonly{#1}}
\newcommand{\refalgrange}[2]{Algorithms~\refalgnumonly{#1}--\refalgnumonly{#2}}

\newcommand{\labfig}[1]{\label{fig:#1}}
\newcommand{\reffignumonly}[1]{\ref{fig:#1}}
\newcommand{\reffig}[1]{Figure~\reffignumonly{#1}}
\newcommand{\Reffig}[1]{Figure~\reffignumonly{#1}}%same as \reffig, but always capitalised!
\newcommand{\refthreefigs}[3]{Figures~\reffignumonly{#1}, \reffignumonly{#2}, and \reffignumonly{#3}}
\newcommand{\reftwofigures}[2]{Figures~\reffignumonly{#1} and \reffignumonly{#2}}
\newcommand{\reffigrange}[2]{Figures~\reffignumonly{#1}--\reffignumonly{#2}}
\newcommand{\refsubfigfrommaincaption}[1]{Figure~\protect\subref{fig:#1}}
\newcommand{\Refsubfigfrommaincaption}[1]{Figure~\protect\subref{fig:#1}}%same, but always capitalized!
\newcommand{\reftwosubfigsfrommaincaption}[2]{Figures~\protect\subref{fig:#1} and ~\protect\subref{fig:#2}}
% \protect\subref{fig:lcbragg_singlenormal_thetan60_decomposed_noanti}

\newcommand{\labtab}[1]{\label{tab:#1}}
\newcommand{\reftabnumonly}[1]{\ref{tab:#1}}
\newcommand{\reftab}[1]{Table~\reftabnumonly{#1}}
\newcommand{\Reftab}[1]{Table~\reftabnumonly{#1}}%same as \reftab, but always capitalised!
%%%%\newcommand{\lablisting}[1]{\label{lst:#1}}
%%%%\newcommand{\reflistingnumonly}[1]{\ref{lst:#1}}
%%%%\newcommand{\reflisting}[1]{Listing~\reflistingnumonly{#1}}
%%%%\newcommand{\Reflisting}[1]{Listing~\reflistingnumonly{#1}}%same as \reflisting, but always capitalised!
\newcommand{\labeqn}[1]{\label{eqn:#1}}
\newcommand{\refeqnnumonly}[1]{\ref{eqn:#1}}
\newcommand{\refeqn}[1]{Eq.~\refeqnnumonly{#1}}
\newcommand{\refeqnrange}[2]{Eqs.~\refeqnnumonly{#1}--\refeqnnumonly{#2}}
\newcommand{\Refeqn}[1]{Eq.~\refeqnnumonly{#1}}%same as \refeqn, but always capitalised!
\newcommand{\reftwoeqns}[2]{Eqs.~\refeqnnumonly{#1} and \refeqnnumonly{#2}}
\def\LRA{\Leftrightarrow}
\def\RA{\Rightarrow}

\title{Revisiting Becker-Coppens (1974): Updated Recipes for Estimating Extinction Factors in Spherical Crystallites}

% Authors and affiliations (uses the standard authblk package):
% Author affiliations are indicated by lowercase letters in square brackets in the \author macro.
% Affiliations (referenced by the lowercase letters in square brackets) are listed after all the authors have been defined.
% The email addresses of corresponding/contact authors can be included using:
% \IUCrCemaillink{corrauthor@org.org}
% Other co-author email addresses can be included using:
% \IUCrEmaillink{coauthor@org.org}
% ORCiDs can be included using:
% \IUCrOrcidlink{xxxx-xxxx-xxxx-xxxx}
% Author footnotes can be included using:
% \IUCrAufn{Text...}
% and to apply the same footnote to another author use:
% \IUCrAufn[1]{}
% where the number in square brackets refers to the numerical order of the
% previously defined footnote.
% For example:
% \author[a]{Anne Author\IUCrCemaillink{corrauthor@org.org}\IUCrOrcidlink{xxxx-xxxx-xxxx-xxxx}}


\author[a]{Thomas Kittelmann\IUCrCemaillink{thomas.kittelmann@ess.eu}\IUCrOrcidlink{0000-0002-7396-4922}}%
\author[b]{Secundus Segunda\IUCrEmaillink{coauthor@org.org}\IUCrOrcidlink{xxxx-xxxx-xxxx-xxxx}\IUCrAufn{Unique note.}}%
\author[a,b]{Trinity Terzi\IUCrEmaillink{anothercoauthor1@org.org}\IUCrOrcidlink{xxxx-xxxx-xxxx-xxxx}\IUCrAufn{Shared note.}}%
\author[a,b]{Clover Dufour\IUCrEmaillink{anothercoauthor2@org.org}\IUCrAufn[2]{}}

\affil[a]{ESS DMSC, European Spallation Source ERIC, Denmark}
\affil[b]{Different Department, Different Organization, ..., Country }

\begin{document}
\maketitle

\begin{synopsis}
A technical correction and enhancement of the Becker-Coppens (1974) recipe for
estimating extinction factors in spherical crystallites is provided, addressing
limitations in scenarios with higher Bragg angles as well as providing improved
precision in general. The work utilises modern computing capabilities and
improved recipes are provided for easy community adoption.
\end{synopsis}

\begin{abstract}
We present a technical correction and update of the widely used recipes by
Becker-Coppens (1974), for the estimation of primary and secondary extinction
factors in perfect spherical domains or particles. In the original publication,
these extinction factors were evaluated numerically from a complicated integral,
and simplified analytical approximations to these numerical evaluations
constituted the provided recipes. However, the original recipes are plagued by
issues of numerical precision in general, and even suffer from complete
numerical break-down in case of strong primary extinction effects in
back-scattering scenarios ($\theta_\texttt{Bragg}\gtrsim50^\circ$). Using modern
computing capabilities, the numerical evaluations of the integrals are
revisited, and improved recipes are provided with precision guarantees for all
Bragg angles and levels of extinction. The new recipes are provided both in a
``standard'' version, believed to be of suitable precision for any actual
analysis of diffraction or transmission data, and a ``luxury'' reference version
with even higher precision.

The performance of the new recipes is naturally compared to those of the
original work, and in order to facilitate easy adoption by the community,
reference implementations are provided for \texttt{C}, \texttt{C++}, and
\texttt{Python} language.
\end{abstract}

\keywords{extinction; Bragg diffraction; neutron scattering; X-ray diffraction; }

\section{The Becker-Coppens recipes for extinction}

Extinction in the context of diffraction refers to the phenomenon where the
intensity of a diffracted wave is reduced or diminished due to interference
effects. The exact nature of these interference effects might be complicated,
but at least for the case of {{\em} secondary} extinction effects, this can
usually be explained by the particle undergoing multiple Bragg scattering's on
the same reflection plane. Each scattering event will alternate the neutron's
orientation between the diffracted and undiffracted directions, with the net
effect being that the diffracted flux is diminished compared to what one would
expect  if multiple scattering effects were not possible. The extinction factor,
$y$, is defined as the difference between the diffracted flux predicted by the
kinematical model ($I_\text{kin}$) with no multiple scattering or other dynamic
effects, and the diffracted flux which is actually observed once these effects
are accounted for ($I_\text{obs}$):
\begin{align}
y \equiv \frac{I_\text{obs}}{I_\text{kin}}
\end{align}
The extinction factor $y$, is a real number in the unit interval which in
general will depend on both the state of the particle undergoing diffraction as
well as of the details of the material in which the process takes place.  In all
cases, $y$ is expressed as a function of both the Bragg angle $\theta$ as well
as a parameter $x$ which capture the general strength of extinction effects in
the material. The exact definition of $x$ depends to some degree on the specific
model, but in general it will be proportional to $(Fl\lambda/V)^2$ where $F$ is
the structure factor of the crystal reflection in question, $V$ the volume of
the crystal unit cell, $\lambda$ the wavelength of the particle being scattered,
and $l$ some relevant length scale representing the path length of the probe
through the coherent domains, crystallites, or grains in which the diffraction
is taking place. In the case where the extinction is governed by a mosaic
distribution, the definition of $x$ will usually also depend on parameters of
this mosaic distribution, such as in particular its width.

Several models for $y$ has been proposed over the years, with early work by
Zachariasen \cite{zachariasen1967} improved upon by other authors like
Cooper-Rouse \cite{cooper1970extinction}, Becker-Coppens \cite{BC1974}, and
Sabine \cite{sabine2006flow}. Of these, the Becker-Coppens model is arguably the
more highly referenced and utilised work, so it was naturally considered by the
authors when working on implementing extinction models in \texttt{NCrystal}
\cite{ncrystalmain,ncrystalelastic} in order to facilitate the modelling and
analysis of Bragg edge transmission and diffraction data. (TODO: Cite Be
extinction paper here?  But only if it also considered BC and not just
Sabine?). Unfortunately, in case of stronger extinction effects, the model
appears to suffer from an obvious breakdown at large Bragg angles, as can be
seen in \reffig{ypbreakdownorigrecipe}.

\begin{figure}[tp] %
\begin{center}
\includegraphics[width=0.6\textwidth]{../data/ncrystal_bc_yp_breakdown_be.pdf}
\end{center}
\caption{Bla. Beryllium.} % NB \protect\cite{...} is required in floating figures
\labfig{ypbreakdownorigrecipe}
\end{figure}

As will be shown in the present work, this breakdown does not constitute a
fundamental problem with the Becker-Coppens model, but is merely a result of
flawed numerical evaluations entering into the recipes (algorithms) provided in
that work. It is the intent of the present work to provide updated recipes which
does not change the underlying theory or assumptions going ito the Becker-Coppens
model, but merely ensures a consistent and precise numerical evaluation of
it. These new recipes for evaluation of $y$ will be referred to as BC2025 in the
present work, while the recipes from the original work will be referred to as
BC1974. It will be seen that the $y$ values provided by the BC2025 recipes do
not drastically disagree agree with those of the BC1974 recipes in the regions
of intermediate extinction levels, where the model has arguably seen most usage
and validation over the years in X-ray or neutron diffraction.

In the work by Becker-Coppens \cite{BC1974}, four specific models and recipes
are provided for the determination of $y$: one for primary extinction, and three
for secondary extinction, where the three latter models only differs in the
choice of distribution used to describe the mosaic spread of coherent domains
within the crystalline materials: Gaussian, Lorentzian, or Fresnellian.

\subsection{Mathematical formulation of the models}

Readers are referred to the original work \cite{BC1974} for the detailed
description and background of the models, while the present work will focus
exclusively on the mathematical expressions from the models which must be
evaluated. To determine the extinction parameter $y$ as a function of $x$ and
$\theta$, the following integral must be evaluated:
\begin{align}
  y_M(\theta,x) = \frac{6c_M}{4\pi}\int_{0}^{\infty} f_M(\eta)\varphi\left(\theta,c_Mxf_M(\eta)\right)d\eta
  \labeqn{yintegral}
\end{align}
Here, the subscript $M$ indicates the model, and can either be $P$
(primary), $G$ (secondary Gaussian), $L$ (secondary Lorentzian), or $F$
(secondary Fresnellian). The variable $c_M$ is a constant, with model dependent
values of $c_P=c_G=c_F=1$ and $c_L=4/3$.

TODO?: Note that the definition of $x$ itself also depends on the model, but we will
drop the index on $x$ for clarity.
TODO? (note we have used that $f(\eta)$ is an even function to change the integration
domain to $[0,\infty)$).


The function $\varphi(\theta,s)$ is an extinction correction function given by:
\begin{align}
  \varphi(\theta,s) = \varphi^0(s) + \sin^{3/2}\theta\left\{\varphi^\pi\left(s\sqrt{\sin\theta}\right)-\varphi^0\left(s\sqrt{\sin\theta}\right)\right\}
  \labeqn{phi}
\end{align}
Here $\varphi^0(s)$ and $\varphi^\pi(s)$ are the limiting functions for $\theta=0$
and $\theta=\pi/2$ respectively, and are given as:\footnote{Note that the expression for
$\varphi^\pi(s)$ in eq. 32 of \cite{BC1974} has a mistake, where a factor of $1/2$
should be moved from the second to the third term.}
\begin{align}
  \varphi^0(s) &= \frac{3}{64s^3}\left\{8s^2+4s\exp(-4s)-(1-\exp(-4s))\right\}\\
  \varphi^\pi(s) &= \frac{3}{4s^3}\left\{s^2-s+\frac{1}{2}\log(1+2s)\right\}
  \labeqn{phizeroandphipi}
\end{align}
All the $\varphi$--functions are even functions of $s$, and as can be seen in
\reffig{phiofs}, monotonically decreasing functions of $|s|$, attaining
values $1$ at $s=0$ and $0$ at $s=\pm\infty$.
\begin{figure}[tp] %
\begin{center}
\includegraphics[width=0.6\textwidth]{generated/phi_of_s.pdf}
\end{center}
\caption{Bla.} % NB \protect\cite{...} is required in floating figures
\labfig{phiofs}
\end{figure}

The $f_M(\eta)$ functions, are essentially unit-less functions capturing the
shape of the Bragg diffraction cross section as a function of deviation from the
nominal direction, normalised so $f_M(0)=1$. The difference in extinction levels
predicted by the different Becker-Coppens models are essentially due to the
difference of these $f_M(\eta)$ functions. For the secondary extinction models,
the shape of $f_M(\eta)$ are essentially given by the mosaic distribution of coherent
crystal domains within material particles, while for primary extinction it is
due to the intrinsic response of a crystalline domain which is perfect but of
finite size. Additionally, the exact final form of the $f_M(\eta)$ functions are
given based on spherical geometries for the crystallites or particles. The exact
form of these functions, plotted in \reffig{fofeta}, are as follows:
\begin{figure}[tp] %
\begin{center}
\includegraphics[width=0.6\textwidth]{generated/f_of_eta.pdf}
\end{center}
\caption{Bla.} % NB \protect\cite{...} is required in floating figures
\labfig{fofeta}
\end{figure}
\begin{subequations}
  \begin{align}
    f_P(\eta) &= \frac{\eta^2-\eta\sin(2\eta)+sin^2(\eta)}{\eta^4}\labeqn{fofeta:P:first}\\
    f_G(\eta) &= \exp\left\{-\frac{9}{16\pi}\eta^2\right\}\labeqn{fofeta:G}\\
    f_L(\eta) &= \frac{1}{1+\eta^2}\labeqn{fofeta:L}\\
    f_F(\eta) &= \frac{ \sin^2(3\eta/4) } { (3\eta/4)^2 }\labeqn{fofeta:F:last}
  \end{align}
\end{subequations}

The task of providing updated recipes for the evaluation of \refeqn{yintegral}
is twofold. Firstly, a reference dataset must be created by
evaluating the integral to high precision on a sufficiently large number of
$(x,\theta)$ points. This is the subject of \refsec{refeval}.
Secondly, a recipe must be found and tuned on this dataset in order to
reproduce its values with suffiently high precision. As such recipes would
ultimately be used in data modelling or analysis, it should be possible to
implement them in software with a reasonably low cost of
evaluation. Fortunately, computing hardware has evolved tremendously since the
original recipes by Becker-Coppens were developed, and some additional degree of
complexity in the updated recipes are therefore allowed.

One important issues in all of this is the definition of precision of a given
estimate of $y$, compared with a reference value $y_\text{ref}$. The naive
definition $\text{precision}(y)\equiv\frac{|y-y_\text{ref}|}{y_\text{ref}}$ is
not always useful at low levels of extinction where $y$ values are always close
to $1$, and where the quantity distinguishing different models might arguably be
$1-y$ rather than $y$ itself. To ensure that the developed recipes for
determining $y$ are useful and trustworthy in all scenarios, the more stringent
definition of precision will thus be used throughout this work:
\begin{align}
  \text{precision}(y) \equiv \frac{|y-y_\text{ref}|}{\min(y_\text{ref},1-y_\text{ref})}
\end{align}
Naturally, for this definition to be useful in practice, this requires
$y_\text{ref}$ to itself be available in a precision much higher than the
precision on $y$.

\section{Evaluating the integrals}\labsec{refeval}

TODO: Mention somewhere the target precision, e.g. that we have 1e-8 (check)
everywhere, and mostly a lot better since a conservative approach to error
estimation is employed everywhere.

The behaviour of the integrand of \refeqn{yintegral} is illustrated in
\reffig{integrandfcts}, and is seen to carry over features from the equivalent
$f(\eta)$ functions in \refeqnrange{fofeta:P:first}{fofeta:F:last}. In
particular two of these features are challenging from the point of view of
numerical quadrature: Slow decay ($\sim1/\eta^2$) and oscillatory behaviour. Only
the Gaussian model ($M=G$) exhibits neither of these, and is therefore
relatively straight-forward to evaluate via numerical quadrature.
\begin{figure}[tp]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/integrand_gPgGgLgF_of_eta_g.pdf}
        \caption{}\labfig{integrandfcts:subfig:gPGLF}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/integrand_gP_of_eta_g.pdf}
        \caption{}\labfig{integrandfcts:subfig:gP}
    \end{subfigure}
    \caption{\Refsubfigfrommaincaption{integrandfcts:subfig:gPGLF} shows the
      integrand of \refeqn{yintegral} for the four different models at
      $x=1$ and $\theta=45^\circ$. \Refsubfigfrommaincaption{integrandfcts:subfig:gP}
      shows the integrand for the primary extinction model ($M=P$) at various values of
      $x$ and $\theta$.}
    \labfig{integrandfcts}
\end{figure}
%see \reffig{integrandfcts:subfig:gP} and \reffig{integrandfcts:subfig:gPGLF} and \reffig{integrandfcts}.

In order to reduce the concern of floating point precision, all work described
in this section is carried out in \texttt{Python} using the arbitrary-precision
floating point arithmetic provided by the \texttt{mpmath} package \cite{mpmath},
configured to keep at least 150 digits of precision in all
operations. Additionally, the implementation of the $\varphi^0(s)$,
$\varphi^\pi(s)$, and $f_M(\eta)$ functions were all implemented in a
numerically stable manner. Specifically, all special functions are evaluated
with special \texttt{mpmath} routines where appropriate (including the $\sinc$
function for $f_F(\eta)$), and the $f_P(\eta)$, $\varphi^0(s)$, and
$\varphi^\pi(s)$ functions are evaluated using Taylor expansions of at least 50
orders when their arguments are small, since their direct mathematical forms are
otherwise numerically unstable in that region. The Taylor expansions are found with the
help of the \texttt{SageMath} \cite{sagemath} mathematics software system.
\footnote{For reference, all the code and datasets associated with the present
work are provided in (FIXME: ZENODO??), and can be run completely with a
Conda-Forge(REF?) installation of purely Open Source tools. The main software
used is \texttt{Python}, \texttt{mpmath}, \texttt{SageMath}, \texttt{NumPy} and
\texttt{matplotlib} (FIXME REFS) (FIXME: Mention data as json??).}

The actual numerical quadrature evaluation of the integral is based on
Gauss-Legendre quadrature, and is performed over a large number of sub-intervals
of $\eta$ values in order to ensure the highest precision of results. This
approach works well for reasonable $x$ values, say
$0.001{\lesssim}x{\lesssim}1000$, with the exact limits depending on the model
$M$. For practical reasons, we shall limit $x{\le}1000$, and simply provide a
reasonable extrapolation to higher $x$ values in the final recipes (TODO: ref to
highx section). This is considered adequate since $x>1000$ is associated with
such hefty levels of extinction that almost no scattering cross section remains
anway. Also note in this regard that the original Becker-Coppens recipes only
provided numerical reference values for $0.1{\le}x\le30$). For low $x$ values it
is fortunately the case that it has been possible to find Taylor series
approximations to the exact result near $x=0$ using \texttt{SageMath}. This
result not only provides convenient high-precision results near $x=0$, but also
serves as a valuable verification of the numerical quadrature results in the
overlapping region where both methods can be used. For $M=P$, $G$, $L$, and $F$
respectively, 30, 60, 60, and 20 orders are used in the Taylor expansions. The
sum of the absolute value of the last 3 terms in the expansions are used as a
very conservative estimate on the error of these results. For reference, the
first five orders of the Taylor expansions are shown here:
\begin{align}
  y_P(x,\theta)\approx&\,  1 - \tfrac{33}{35}x
+ \tfrac{2369}{5775}(\sin^{5/2}\theta + 2)x^2
 - \tfrac{54120476}{91216125}(1+2\sin^3\theta)x^3\labeqn{ytaylor:P:first}\\\nonumber
&+ \tfrac{350769113251}{1924903480500}(2+13\sin^{7/2}\theta)x^4
 - \tfrac{276478446363113}{2846107289025000}(2+43\sin^4\theta)x^5\\
  y_G(x,\theta)\approx&\,  1 - \tfrac{3\sqrt{2}}{4}x
  + \tfrac{4\sqrt{3}}{15}(2+\sin^{5/2}\theta)x^2
- \tfrac{2}{3}(1+2\sin^3\theta)x^3\labeqn{ytaylor:G}\\\nonumber
 &+ \tfrac{16\sqrt{5}}{175}(2+13\sin^{7/2}\theta)x^4
 - \tfrac{2\sqrt{6}}{45}(2+43\sin^4\theta)x^5\\
  y_L(x,\theta)\approx&\,  1-x
  + \tfrac{8}{15}(2+\sin^{5/2}\theta)x^2
- \tfrac{80}{81}(1+2\sin^3\theta)x^3\labeqn{ytaylor:L}\\\nonumber
&+ \tfrac{32}{81}(2+13\sin^{7/2}\theta)x^4
- \tfrac{112}{405}(2+43\sin^4\theta)x^5\\
  y_F(x,\theta)\approx&\, 1 - x
  + \tfrac{11}{25}(2+\sin^{5/2}\theta) x^2
  - \tfrac{604}{945}(1+2\sin^3\theta)x^3 \labeqn{ytaylor:F:last}\\\nonumber
  &+ \tfrac{15619}{79380}(2+13\sin^{7/2}\theta)x^4
   - \tfrac{655177}{6237000}(2+43\sin^4\theta)x^5
\end{align}

The next issue is that it is obviously not practically possible to perform the
numerical quadrature over $\eta$ all the way to infinity. For $M=G$ the
contributions decreases so strongly with $\eta$, that there is never any
significant contribution to the integral for $eta>1000$, and accordingly
numerical quadrature out to $\eta=1000$ is always sufficient to ensure the
desired precision on $y$ values. For $M=L$, the integrand is sufficiently simple
that we are able to find a Taylor series approximation for the tail of the
integrand, so that we get:
\begin{align}
\int_{a}^{\infty} f_L(\eta)\varphi\left(\theta,xf_L(\eta)\right)d\eta \approx
\frac{1}{a} - \frac{2k}{3a^2}
+\left(
\frac{128 + 64{\sin^{5/2}\theta}}{225}k^2
-\frac{1}{3}
\right)\frac{1}{a^3} + \ldots
\labeqn{tailintegralL}
\end{align}
where we have required $a>x>0$ and $a>1$, and introduced $k=x/a$ which satisfies
$0<k<1$. For breweity, only the first few terms are shown in
\refeqn{tailintegralL}, but in the actual implementation, terms up to $1/a^{20}$
are included. Again, the error bound is conservatively estimated as the sum of
the absolute values of the last three terms (of order $1/a^{18}$, $1/a^{19}$,
and $1/a^{20}$). Thus, in practice the error bound will be of order $1/a^{18}$,
requiring actual numerical quadrature only over $[0,a]$ for very modest values
of $a$. For $M=P$ the integrand is more challenging due to the presence of
trigonometric functions. Firstly, the desire for high precision results implies
that the $\eta$ ranges passed into the numerical quadrature machinery will be
limited to always be shorter than the period of these trigonometric functions,
with a resulting large increase in the number of times the Gauss-Legendre
quadrature algorithm must be applied. Secondly, the increased complexity of the
integrand means that unlike the case of $M=L$, we are not able to find a direct
Taylor series approximation for the tail integral from $[a,\infty]$. However, as
is shown in appendix TODOTODO, we are able to bound $f_P(\eta)$ by simpler
functions, and accordingly obtain a useful bound on the tail contribution in
this case as well:
\begin{align}
\int_{a}^{\infty} f_P(\eta)\varphi\left(\theta,xf_P(\eta)\right)d\eta \approx
\frac{1}{a}  - \frac{x}{2a^3 } \pm\left(\frac{1}{2a^2 }+\frac{2}{a^3}\right),\quad{}a>10,\,a>x>0
\labeqn{tailintegralP}
\end{align}
Here the indicated error is a conservative upper bound on the error, and
unfortunately the slower convergence of just $\order(1/a^2)$ implies that actual
numerical quadrature is required over $[0,a]$ for rather significant values of
$a$ -- but fortunately not impossible on modern hardware.

Finally, $M=L$ poses a somewhat different challenge, due to the fact that it was
not possible to find a sufficiently simple functions bounding $f_L(\eta)$ tight
enough to provide a useful estimate of the contribution of the tail of the
integral. Since the integrand additionally decays only as $1/\eta^2$ and suffers
from the same oscillations as $f_P(\eta)$, it is impractical to simply integrate
over $[0,a]$ for a sufficiently large $a$. Thus, a different approach was used
in this case, instead defining the $n$th contribution as the integral over the
interval $[(4\pi/3)n,(4\pi/3)(n+1)]$, find the contributions up to at least
$n=1000$ and then use Richardson's extrapolation \cite{richardson1911ix} via
\texttt{mpmath}'s \texttt{nsum} function to include the contributions up
to $n=\infty$.

TODO: Show resulting rainbow curves here? And consider putting the details of
the evaluations (next 4 subsections) in appendix?
Bla \reffig{yrefeval}.

%\begin{align}
%  y_P(x,\theta)\approx&\,  1 - \tfrac{33}{35}x
%+ \tfrac{2369}{5775}(\sin^{5/2}\theta + 2)x^2
% - \tfrac{54120476}{91216125}(1+2\sin^3\theta)x^3\\\nonumber
%&+ \tfrac{350769113251}{1924903480500}(2+13\sin^{7/2}\theta)x^4
% - \tfrac{276478446363113}{2846107289025000}(2+43\sin^4\theta)x^5 \\
%%& + \tfrac{19564394058822418571}{855860038150930312500}(4+311\sin^{9/2}\theta)x^6\\
%%
%  y_G(x,\theta)\approx&\,  1 - \tfrac{3\sqrt{2}}{4}x
%  + \tfrac{4\sqrt{3}}{15}(2+\sin^{5/2}\theta)x^2
%- \tfrac{2}{3}(1+2\sin^3\theta)x^3\\\nonumber
% &+ \tfrac{16\sqrt{5}}{175}(2+13\sin^{7/2}\theta)x^4
% - \tfrac{2\sqrt{6}}{45}(2+43\sin^4\theta)x^5\\
%% +\tfrac{64\sqrt{7}}{6615}(4+311\sin^{9/2}\theta)x^6\\
% %
%  y_L(x,\theta)\approx&\,  1-x
%  + \tfrac{8}{15}(2+\sin^{5/2}\theta)x^2
%- \tfrac{80}{81}(1+2\sin^3\theta)x^3\\\nonumber
%&+ \tfrac{32}{81}(2+13\sin^{7/2}\theta)x^4
%- \tfrac{112}{405}(2+43\sin^4\theta)x^5\\
%% +\tfrac{2816}{32805}(4+311\sin^{9/2}\theta)x^6\\
%%
%  y_F(x,\theta)\approx&\, 1 - x
%  + \tfrac{11}{25}(2+\sin^{5/2}\theta) x^2
%  - \tfrac{604}{945}(1+2\sin^3\theta)x^3 \\\nonumber
%  &+ \tfrac{15619}{79380}(2+13\sin^{7/2}\theta)x^4
%   - \tfrac{655177}{6237000}(2+43\sin^4\theta)x^5
%%  + \tfrac{27085381}{1094593500}(4+311\sin^{9/2}\theta)x^6
%\end{align}

%%%\subsection{Details of evaluation for $M=P$}\labsec{refeval:subsectionP}
%%%
%%%show lowx taylor for a few terms, plus mention how many terms used.
%%%\subsubsection{Taylor series approximation}
%%%
%%%Taylor orders used for evaluation:
%%%P 30
%%%G 60
%%%L 60
%%%F 20
%%%
%%%\begin{align}
%%%  y_P(x,\theta)\approx&\,  1 - \tfrac{33}{35}x
%%%+ \tfrac{2369}{5775}(\sin^{5/2}\theta + 2)x^2
%%% - \tfrac{54120476}{91216125}(1+2\sin^3\theta)x^3\\\nonumber
%%%&+ \tfrac{350769113251}{1924903480500}(2+13\sin^{7/2}\theta)x^4
%%% - \tfrac{276478446363113}{2846107289025000}(2+43\sin^4\theta)x^5
%%%\end{align}
%%%
%%%
%%%\subsubsection{bla bla quads}
%%%
%%%Mention specifics of quad intervals (bounds).
%%%
%%%\subsection{Details of evaluation for $M=G$}\labsec{refeval:subsectionG}
%%%
%%%\subsubsection{Taylor series approximation}
%%%
%%%show lowx taylor for a few terms, plus mention how many terms used.
%%%
%%%\begin{align}
%%%  y_G(x,\theta)\approx&\,  1 - \tfrac{3\sqrt{2}}{4}x
%%%  + \tfrac{4\sqrt{3}}{15}(2+\sin^{5/2}\theta)x^2
%%%- \tfrac{2}{3}(1+2\sin^3\theta)x^3\\\nonumber
%%% &+ \tfrac{16\sqrt{5}}{175}(2+13\sin^{7/2}\theta)x^4
%%% - \tfrac{2\sqrt{6}}{45}(2+43\sin^4\theta)x^5\\
%%%\end{align}
%%%
%%%
%%%Mention no tail needed!!
%%%
%%%Mention specifics of quad intervals (bounds).
%%%
%%%
%%%\subsection{Details of evaluation for $M=L$}\labsec{refeval:subsectionL}
%%%
%%%\subsubsection{Taylor series approximation}
%%%
%%%show lowx taylor for a few terms, plus mention how many terms used.
%%%
%%%\begin{align}
%%%  y_L(x,\theta)\approx&\,  1-x
%%%  + \tfrac{8}{15}(2+\sin^{5/2}\theta)x^2
%%%- \tfrac{80}{81}(1+2\sin^3\theta)x^3\\\nonumber
%%%&+ \tfrac{32}{81}(2+13\sin^{7/2}\theta)x^4
%%%- \tfrac{112}{405}(2+43\sin^4\theta)x^5\\
%%%\end{align}
%%%
%%%\subsubsection{Tail integration}
%%%
%%%NOTE: Here we can actually find a closed-form taylor expression in $1/a$. With
%%%$k=x/a$ and requiring $a>x>0$ and $a>1$. Showing for brewity just the first
%%%three orders of $1/a$ here, it is:
%%%\begin{align}
%%%\int_{a}^{\infty} f_L(\eta)\varphi\left(\theta,xf_L(\eta)\right)d\eta \approx
%%%\frac{1}{a} - \frac{2k}{3a^2}
%%%+\left(
%%%\frac{128 + 64{\sin^{5/2}\theta}}{225}k^2
%%%-\frac{1}{3}
%%%\right)\frac{1}{a^3} + \ldots
%%%\labeqn{tailintegralLDUPLICATE}
%%%\end{align}
%%%In the implementation, terms up to $1/a^{20}$ are included, and the error bound
%%%is conservatively estimated as the sum of the absolute values of the terms of
%%%order $1/a^{18}$, $1/a^{19}$, and $1/a^{20}$. Thus, in practice the error
%%%bound will be of order $1/a^{18}$ which is a lot more precise than the case for
%%%$M=P$ where it only improves as order $1/a^{2}$. Hence, actual numerical
%%%quadrature is only needed over a relatively limited range.
%%%
%%%Mention specifics of quad intervals (bounds).
%%%
%%%\subsection{Details of evaluation for $M=F$}\labsec{refeval:subsectionF}
%%%
%%%\subsubsection{Taylor series approximation}
%%%
%%%show lowx taylor for a few terms, plus mention how many terms used.
%%%
%%%\begin{align}
%%%  y_F(x,\theta)\approx&\, 1 - x
%%%  + \tfrac{11}{25}(2+\sin^{5/2}\theta) x^2
%%%  - \tfrac{604}{945}(1+2\sin^3\theta)x^3 \\\nonumber
%%%  &+ \tfrac{15619}{79380}(2+13\sin^{7/2}\theta)x^4
%%%   - \tfrac{655177}{6237000}(2+43\sin^4\theta)x^5
%%%%  + \tfrac{27085381}{1094593500}(4+311\sin^{9/2}\theta)x^6
%%%\end{align}
%%%
%%%mention tail encapsulation as for M=P does not work, since integrand
%%%continuously goes to y=0. Thus interval required would be insane. So therefore:
%%%robinson extrapolation.
%%%
%%%Mention specifics of quad intervals (bounds).

\begin{figure}[tp]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/yrefeval_primary.pdf}
        \caption{}\labfig{yrefeval:subfig:primary}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/yrefeval_bands.pdf}
        \caption{}\labfig{yrefeval:subfig:bands}
    \end{subfigure}
    \caption{\Refsubfigfrommaincaption{yrefeval:subfig:primary} shows the
      $y_P(x,\theta)$ curves resulting from the high precision evaluation of
      \refeqn{yintegral}, with each color denoting a specific value of $\theta$
      as indicated. \Refsubfigfrommaincaption{yrefeval:subfig:bands}
      shows the bands of $y_M(x,\theta)$ values covered by all four models, as
      $\theta$ is varied from $0^\circ$ to  $90^\circ$..}
    \labfig{yrefeval}
\end{figure}

\section{Original recipes}\labsec{origrecipes}

Original recipes and their problems:


   - Start by mentioning that we can produce new high-prec integrals and
     recipes, to be discussed in later sections. Here they will be used as
     reference. (In section XXX we will discuss how the integral in \refeqn{yintegral}....)
   - table 1/3/4
   - the actual recipes
   - the breakdown plots

\begin{align}
  y_{p}(\theta,x)=\left\{1+2x+\frac{A(\theta)x^2}{1+B(\theta)x}\right\}
  \labeqn{ypfitfctorig}
\end{align}

\begin{align}
  A(\theta) &= 0.20 + 0.45\cos(2\theta) \\
  B(\theta) &= 0.22 - 0.12(0.5-\cos(2\theta))^2
  \labeqn{ypfitfctparsorig}
\end{align}

\begin{figure}[tp]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/prec_classicrecipe_2d_primary.pdf}
        \caption{$M=P$}\labfig{prec2dplotNEW:subfig:P}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/prec_classicrecipe_2d_scndgauss.pdf}
        \caption{$M=G$}\labfig{prec2dplotNEW:subfig:G}
    \end{subfigure}
    \vspace{0.5cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/prec_classicrecipe_2d_scndlorentz.pdf}
        \caption{$M=L$}\labfig{prec2dplotNEW:subfig:L}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/prec_classicrecipe_2d_scndfresnel.pdf}
        \caption{$M=F$}\labfig{prec2dplotNEW:subfig:F}
    \end{subfigure}
    \caption{Bla. precision 2d classic recipe (vs. new lux recipe)}
    \labfig{prec2dplotNEW}
\end{figure}

tables: Mention OCR(tesseract)+human checking.

\begin{table}[tp]
\caption{Precision of original BC Table 1 compared to updated values (NB: MENTION PRECISION DEF USED).}
\smallskip
\tiny
\begin{center}
\input{generated/table1_diff.tex}
\end{center}
\end{table}

\begin{table}[tp]
\caption{Precision of original BC Table 3 compared to updated values (NB: MENTION PRECISION DEF USED).}
\smallskip
\tiny
\begin{center}
\input{generated/table3_diff.tex}
\end{center}
\end{table}

\begin{table}[tp]
\caption{Precision of original BC Table 4 compared to updated values (NB: MENTION PRECISION DEF USED).}
\smallskip
\tiny
\begin{center}
\input{generated/table4_diff.tex}
\end{center}
\end{table}


%%\begin{figure}[tp] %
%%\begin{center}
%%\includegraphics[width=0.6\textwidth]{generated/prec_classicrecipe_2d_primary.pdf}
%%\end{center}
%%\caption{precision 2d classic recipe (vs. new lux recipe). This is primary.}
%%\labfig{prec2dplot:primary}
%%\end{figure}
%%
%%\begin{figure}[tp] %
%%\begin{center}
%%\includegraphics[width=0.6\textwidth]{generated/prec_classicrecipe_2d_scndgauss.pdf}
%%\end{center}
%%\caption{precision 2d classic recipe (vs. new lux recipe). This is scndgauss.}
%%\labfig{prec2dplot:scndgauss}
%%\end{figure}
%%
%%\begin{figure}[tp] %
%%\begin{center}
%%\includegraphics[width=0.6\textwidth]{generated/prec_classicrecipe_2d_scndlorentz.pdf}
%%\end{center}
%%\caption{precision 2d classic recipe (vs. new lux recipe). This is scndlorentz.}
%%\labfig{prec2dplot:scndlorentz}
%%\end{figure}
%%
%%\begin{figure}[tp] %
%%\begin{center}
%%\includegraphics[width=0.6\textwidth]{generated/prec_classicrecipe_2d_scndfresnel.pdf}
%%\end{center}
%%\caption{precision 2d classic recipe (vs. new lux recipe). This is scndfresnel.}
%%\labfig{prec2dplot:scndfresnel}
%%\end{figure}
%%


\section{Developing new recipes}\labsec{newrecipes}

With the availability of reliable high-precision reference datasets of
$y_M(x,\theta)$ values discussed in \refsec{refeval}, the goal is now to provide
a set of improved recipes tuned to this data -- in order to replace the original
ones by Becker-Coppens discussed in \refsec{origrecipes}. For the benefit of
future reference work, two version of the recipes will be provided: ``standard''
versions, providing $y$ estimates with errors guaranteed to be less than
$10^{-3}\cdot\min(y_\text{ref},1-y_\text{ref})$ for all $x{\le}1000$ and
``luxury'' versions where the error guarantee is tightened to be less than
$10^{-6}\cdot\min(y_\text{ref},1-y_\text{ref})$. For $x>1000$, no strict error
guarantee is given, but the recipes should still be reasonable and well behaved.

In order to develop these recipes, we first observe that one does not
strictly need to parameterise $y_M(x,\theta)$ as a two-dimensional function as
was done in the original work. This is because it follows from
\reftwoeqns{yintegral}{phi} and the usual rules of integration that:
\begin{align}
  y_M(\theta,x) = y^0_M(x) + {\sin^{3/2}\theta}\,y^\Delta_M(x\sqrt{\sin\theta})
  \labeqn{yintegralbyy0ydelta}
\end{align}
where we have defined
\begin{align}
y^0_M(x)&\equiv y_M(x,0)\\
y^\pi_M(x)&\equiv y_M(x,\pi/2)\\
y^\Delta_M(x)&\equiv y_M^\pi(x)-y_M^0(x)
  \labeqn{y0pideltadef}
\end{align}
Thus, the recipe simply has to contain parameterisations of the two
one-dimensional functions $y^0_M(x)$ and $y^\Delta_M(x)$, and then implement the
$\theta$ dependency via \refeqn{yintegralbyy0ydelta}. In this respect it is
interesting to note that in most envisioned use-cases, it is $\sin\theta$ rather
than $\theta$ which is more cheaply calculated, due to its appearance in the
Bragg condition. This therefore fits nicely with \refeqn{yintegralbyy0ydelta},
where only a single additional square-root must be evaluated rather than a more
expensive trigonometric function. Thus, our recipes will in practice be taking
$\sin\theta$ rather than $\theta$ as argument.

In terms of the $x$-dependency of $y^0_M(x)$ and $y^\Delta_M(x)$, three
different regimes shall be considered. Firstly, the regime at very low
$x$-values is natural to simply evaluate via Taylor expansions. For this
purpose, it was chosen to use 5th order expansions for both $y^0_M(x)$ and
$y^\Delta_M(x)$, coefficients of which can trivially be determined from
\refeqnrange{ytaylor:P:first}{ytaylor:F:last}. The standard recipes will use the
Taylor expansions when $x<0.1$, while the luxury recipes will do so only when
$x<0.02$.

At the other extreme, $x$ values higher than $1000$ will as previously mentioned
simply be handled by extrapolation. As shown in \reffig{highx}, a simple power
law $y_M(x){\sim}x^{-p}$ appears to describe the curves well for $x\gtrsim100$,
so while the new recipes will not provide any precise statement on guaranteed
precision for $x>1000$, they will still provide numerically stable and
reasonable results. The power constant is $p=0.933$ in case of the Gaussian
model ($M=G$), and $p=0.5$ in all other cases.
\begin{figure}[tp] %
\begin{center}
\includegraphics[width=0.6\textwidth]{generated/highx.pdf}
\end{center}
\caption{Reference $y^0_M(x)$ and $y^\pi_M(x)$ curves. Dotted lines show a power
  law $y_M(x){\sim}x^{-p}$ with normalisation fixed to coincide with reference
  curves at $x=1000$. The power constant used is $p=0.5$ when $M{\ne}G$ and
  $p=0.933$ when $M=G$.}
\labfig{highx}
\end{figure}

That leaves just the regime of intermediate $x$-values, ranging from the Taylor
threshold at $x=0.1$ or $x=0.02$, and up to $x=1000$. In order to be able to
perform a standard fit of Legendre polynomials to the data, the $x$ values are
first mapped onto the interval from $[-1,1]$ via the transformation:
\begin{align}
  x' = \frac{\sqrt{x}-1}{\sqrt{x}+1}
  \labeqn{xprimetrf}
\end{align}
Here, any power of $x$ would have worked in place of $\sqrt{x}$, but in practice
the subsequent fits are found to more easily align with the data points for this
specific transformation. Likewise, the fits for $y^0_M(x)$ curves also performed
better when the $y$ values are transformed:
\begin{align}
  y' =  \frac{ y + \frac{1}{4}x'(3 - {x'}^2) -\frac{1}{2}}{ ( 1 + x' )^2( 1 - x' ) }
  \labeqn{yprimetrf}
\end{align}
Legendre polynomials are then fitted to the resulting sets of $(x',y')$ points.
The specific form of the $x'$ polynomium in the numerator of \refeqn{yprimetrf}
was chosen to ensure $y'=0$ at $x'=\pm1$ without changing the slope at either of
these points. The denominator was chosen to factor out this boundary condition,
and again the specific powers used in the denominator was chosen to be those
found to make the Legendre polynomials more easily align with the data
points. Although the fits are performed using Legendre polynomials, due to their
many advantages (such as ortogonality and having fitting procedures readily
available in \texttt{NumPy}FIXMEREF), the final provided recipes will naturally
first convert the fitted coefficients into coefficients of standard
polynomials. This not only simplifies the final recipes, but also allows for
efficient and robust evaluation of these polynomials via Horner's
method(FIXMEREF). Additionally, the specific form of the transformation in
\refeqn{yprimetrf} has the advantage that the inverse transformation from $y'$
values back to $y$ values involves polynomials of $x'$ which can simply be
combined with the polynomials from the legendre fits into a single set of
coefficients.

The fits for $y^\Delta_M(x)$ curves are also performed after a transformation of
$x$ values according to \refeqn{yprimetrf}, but do not, however, require a
transformation of $y$ values akin to \refeqn{yprimetrf}. Otherwise the procedure
is similar to the $y^\Delta_M(x)$ case.

That leaves only the choice of order of Legendre polynomial fit to perform, as
well as the number of digits of the coefficients to include in the final
recipes. In summary, these values were carefully investigated and chosen
independently for each particular $y^0_M(x)$ or $y^\Delta_M(x)$ curve, in order
to live up to the provided error guarantees of the particular recipe, while
naturally keeping the recipe as simple as possible. Putting it all together, the
recipe for $y_P(x,\theta)$ is shown in \refalg{yfct_primary}. All other
recipes follow the same general form, with the main difference being the
specific coefficients used, and for reference the other 7 algorithms are
included in \refappendix{recipes}. Additionally, to further adoption,
\texttt{C}, \texttt{C++}, and \texttt{Python} code implementing the recipes as
standalone functions are also made available at FIXMEREF.
\input{generated/recipe_primary.tex}

\section{Conclusions}

The final precision of both the new ``standard'' and ``luxury'' recipes from the
present work, as well as the original recipes due to Becker-Coppens, are shown
in \reffig{cmprecipes}. It is clearly seen that the new recipes provide the
guaranteed error bounds of respectively $10^{-3}$ and $10^{-6}$, while the older
recipes are mostly useful in a more limited range of intermediate $x$ values.
\begin{figure}[tp]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/cmprecipes_primary.pdf}
        \caption{$M=P$}\labfig{cmprecipes:subfig:P}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/cmprecipes_scndgauss.pdf}
        \caption{$M=G$}\labfig{cmprecipes:subfig:G}
    \end{subfigure}
    \vspace{0.5cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/cmprecipes_scndlorentz.pdf}
        \caption{$M=L$}\labfig{cmprecipes:subfig:L}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/cmprecipes_scndfresnel.pdf}
        \caption{$M=F$}\labfig{cmprecipes:subfig:F}
    \end{subfigure}
    \caption{Bla bla cmprecipes. Note: breakdown near 45degrees in $M=L$ plot (we have split
      that line into $\pm45^\circ$ to show it) }
    \labfig{cmprecipes}
\end{figure}

Although a precise description of extinction phenomenons are vastly complicated
and by no means will be a solved subject due to the present work, it is
nonetheless the hope of the authors that analysis and modelling codes will adopt
these new recipes when applying the Becker-Coppens models, so the research and
discussions going forward can focus on the physics rather than the flawed
numerical evaluations plaguing the original work, which was otherwise extremely
detailed and impressive in the subjective oppinion of the authors..

\appendix % if required

\section{Tail integration primary extinction}
For $M=P$ and $f_P(\eta)$ given by \refeqn{fofeta:P:first}, it is desired to
provide an estimate of the tail of the integral in \refeqn{yintegral}, i.e.\ the
integral over just the $[a,\infty]$ for some finite and positive $a$.

For positive $\eta$ values we have:
\begin{align}
  f_P(\eta) \le f_P^+(\eta) \equiv \frac{\eta^2+\eta+1}{\eta^4}\labeqn{fplus}\\
  f_P(\eta) \ge f_P^-(\eta) \equiv \frac{\eta^2-\eta}{\eta^4}\labeqn{fminus}
\end{align}
Where $f^-(\eta)$ is only strictly positive when $\eta>1$, so we will restrict
ourselves to $\eta>2$ (??)

Since $\varphi(\theta,s)$ and $f(\eta)$ decreases monotonically for increasing
$s$ and $\eta$ respectively for all positive arguments, we can conclude that:
\begin{align}
\int_{a}^{\infty} f_P(\eta)\varphi\left(\theta,xf_P(\eta)\right)d\eta \le
\int_{a}^{\infty} f_P^+(\eta)\varphi\left(\theta,xf^-_P(\eta)\right)d\eta \equiv T^+(\theta,x,a) \nonumber\\
\int_{a}^{\infty} f_P(\eta)\varphi\left(\theta,xf_P(\eta)\right)d\eta \ge
\int_{a}^{\infty} f_P^-(\eta)\varphi\left(\theta,xf^+_P(\eta)\right)d\eta \equiv T^-(\theta,x,a)
\end{align}
Using $k=x/a$ and ensuring $a>\max(x,2)$ we have $k<1$.

With the help of \texttt{SageMath}, $T^\pm(\theta,x,a)$ can be evaluated
analytically and expressed as a Taylor expansion in inverse powers of $a$:
\begin{align}
  T^+(\theta,x,a) &= \frac{1}{a}  + \frac{1-k}{2a^2 }
  +\left[\frac{1}{3} + \frac{8+4\sin^{5/2}\theta}{25}k^2  \right]\frac{1}{a^3}
  +\mathcal{O}\left(\frac{1}{a^4}\right)\\
  T^-(\theta,x,a) &= \frac{1}{a}  - \frac{1+k}{2a^2 }
  +\frac{8+4\sin^{5/2}\theta}{25}k^2\frac{1}{a^3}
  +\mathcal{O}\left(\frac{1}{a^4}\right)
\end{align}

Varying both $k$ and $\sin\theta$ over values the unit interval and evaluating
Taylor coefficients for terms up to $1/a^{20}$ numerically, it is seen that the
maximal absolute numerical value of coefficients is always within a factor of
$2.50$ when compared to the previous order. Thus, if we restrict ourselves to
$a>10$, we can conservatively limit the maximal contributions of the terms of
order $1/a^4$ or higher in REFEQNABOVE to be at most $10/a^4$. With that, we can
use $(T^+(\theta,x,a)+T^-(\theta,x,a))/2$ as an estimate of the tail integral
value, while $(T^+(\theta,x,a)-T^-(\theta,x,a))/2$ will be a conservative upper
bound of the error on the estimate. In other words, for $a>\max(10,x)$ we get:
\begin{align}
\int_{a}^{\infty} f_P(\eta)\varphi\left(\theta,xf_P(\eta)\right)d\eta \approx
\frac{1}{a}  - \frac{k}{2a^2 }
  +\left[\frac{1}{6} + \frac{8+4\sin^{5/2}\theta}{25}k^2  \right]\frac{1}{a^3}
  \pm
\left(\frac{1}{2a^2 }+\frac{1}{6a^3}+\frac{10}{a^4}\right)
\labeqn{tailintegralpenultimate}
\end{align}
Where the term in the final parenthesis represents an upper bound on the
error. Due to the leading factor of $1/a^2$ in the error term, there is not much
value in keeping the higher order terms of
\refeqn{tailintegralpenultimate}. Thus, by using $k=x/a$ and $a>10$ and a slight
additional pessimisation of the error bound, we can arrive at the relatively
simple formula for the estimation of the tail of the integral given in
\refeqn{tailintegralP}.:


\section{Updated tables}

FIXME: Perhaps we should also make one for the fresnel model?

\begin{table}[tp]
\caption{Updated BC Table 1 with a few extra columns and rows.}
\smallskip
\tiny
\begin{center}
\input{generated/table1_updated.tex}
\end{center}
\end{table}

\begin{table}[tp]
\caption{Updated BC Table 3 with a few extra columns and rows.}
\smallskip
\tiny
\begin{center}
\input{generated/table3_updated.tex}
\end{center}
\end{table}

\begin{table}[tp]
\caption{Updated BC Table 4 with a few extra columns and rows.}
\smallskip
\tiny
\begin{center}
\input{generated/table4_updated.tex}
\end{center}
\end{table}


\section{The recipes}\labappendix{recipes}.

In \refalgrange{yfct_scndgauss}{yfct_scndfresnel} are shown the
``standard'' precision recipes for calculating $y_G(x,\theta)$, $y_L(x,\theta)$,
and $y_F(x,\theta)$, while the recipe for calculating $y_P(x,\theta)$ was
already shown in \refalg{yfct_primary}. Additionally, intended for
high-precision reference work,
\refalgrange{yfct_primary_lux}{yfct_scndfresnel_lux} show the ``luxury''
recipes for calculating the same four functions, but at an increased level of
precision. FIXMEREF to c/c++/py code?
\input{generated/recipe_scndgauss.tex}
\input{generated/recipe_scndlorentz.tex}
\input{generated/recipe_scndfresnel.tex}
\input{generated/recipe_primary_lux.tex}
\input{generated/recipe_scndgauss_lux.tex}
\input{generated/recipe_scndlorentz_lux.tex}
\input{generated/recipe_scndfresnel_lux.tex}

\section{LEFTOVERS}


 of a particularly challenging numerical
integral....  Furthermore, the numerical evaluation in BC1974 is generally most
precise and useful in regions of intermediate extinction levels, where arguably
most data analysis ... a feature of
numerical precision and domain of validity of the recipes as presented in the
... phenomenologically, the observed Bragg diffraction cross section will thus be reduced by
a factor of $y$

... Although these models describe different
underlying physics, they are in practice mathematically similar and each require
the evaluation of a particular non-trivial integral

TODO: Mention BC highly cited, used for structure refinement, etc.

TODO: Mention y is monotonically decreasing function of x, and increases for
higher thetabragg. Mention limits y=1 at x=0, y=0 and x=inf. This is also
important when discussing the precision definition to use.

NEXT: One paragraph with a few details of the BC models, spherical particles,
primary+secondary, for different secondary mosaic distributions, etc. Then start
introducing equations for phi/phi0/phipi/f/y eq36/... and then introduce the
four models PGLF. Then mention the issues, show the NCrystal plot and the 2d
precision plots.

NB: $\theta$ is the Bragg angle, defined by:
\begin{align}
  \sin\theta = \frac{\lambda}{2d_{hkl}}
  \labeqn{braggequation}
\end{align}

And $x$ is defined by (fixme: is $l$ the sphere radius or tbar..., also this is
for primary):

\begin{align}
  x = \frac{ 2l^2\lambda^2F^2}{3V^2}
  \labeqn{xdef}
\end{align}

$N_P=\frac{3}{4\pi}$, $C_m=1$.
(FIXME: Don't focus first om primary, just do them all from the beginning,
meaning that the previous equation has to have index $M$ and two constants
depending on $M$ (the normalisation factors and the factor in front of x, the
only real difference is for the Lorentzian model I think).




FIXME: How can we refer to these models? Both in this paper and in e.g. NCrystal
where we use them? We can use ``BCK'' (Becker-Coppens-Kittelmann), if such
self-reference is allowed? Or perhaps BC2025(lux|minimal|) vs. BC1974 ?

For ease of reference the three updated recipes provided here, will be referred
to as BCK-minimal, BCK, and BCKLux. The first is as the name implies a minimal,
which only ..... Next, the BCK model aims to provide numerical evaluations at
the $0.1\%$ level, while finally BCKLux provides an even more precise numerical
evaluation at the $10^-8$ level of precision. (define precision!)


NOTE: Mention that this should be evaluated using Horner's method!! (which is
also the default in numpy's polyval and how it has always been done in
NCrystal). (Although Motzkin's method or the Knuth-Eve algorithm is actually
interesting as it can save additional multiplications!!)



\begin{acknowledgements}
The contributions of non-authors etc. should be given here.
\end{acknowledgements}

\begin{funding}
List funding organizations, recipients, grant numbers, etc.
\end{funding}

\ConflictsOfInterest{Please declare any conflicts of interest, or declare  that there are no conflicts of interest.
}

\DataAvailability{Please state how the data supporting the results reported in your article can be accessed, e.g. within the article, as published supporting material, in repositories, upon request...
}

\bibliography{iucr} % basename of .bib file

\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%BC x:
%
%r = radius of ideal spherical crystal
%
%x = sigma(0)*r = 3/4 Q*beta*r = 2/3 Q alphabar tbar
%    alphabar = (3/2)r sin2theta/lambda
%    beta = 2r sin2theta / lambda
%   so x = 3/4 Q* 2r sin2theta / lambda  *r = 3/2 Q * r^2 * Q * sin2theta
%   /lambda =
%    Q = ?

%  (3/2)*sphere_r^2 = (3/2)*(AVPATHLENGTH*3/4)^2

%
%    NCrystal code:
%%    bc_x =         wl^2*F^2/V^2 * ( 2 / 3 ) * blksz^2
%%    sabine_x =     wl^2*F^2/V^2 * blksz^2
%
%

%sabine_x =

%block_size =

% Average path length through a sphere of radius is (4/3)*radius = (2/3)*diameter.
% so radius = 3/4 * <average_pathlength>

%If we use blksz_Aa to mean the Average path length through the crystallite, we
%have to use r = 4/3 * ....

% BC x: factor of (2/3) is when using l to mean the average path length through
% the crystallite.

%Sabine x:


% shuqi BC xp:
% double Q_theta = NC::ncsquare(Nc * wl * F_hkl) * wl; //division by sin_2theta to be done later

% double xp = 2. / 3. * NC::ncsquare(Nc * wl * F_hkl) * l * l;



%BC x:
%
%35a:
%
%x = (2/3)*Q*alphabar*tbar
%alphabar = tbar * sin2theta/wl (eq. 35b with 3r/2 = tbar)
%tbar: average path length through the crystallite (tbar=3/2*radius for a sphere).
%     Q = |F/V|^2*wl^3/sin2theta
%
%so x = (2/3)*(|F/V|^2*wl^3/sin2theta)*(tbar * sin2theta/wl)*tbar
%     = (2/3)* |F/V|^2*wl^2 * tbar^2
%
%
% Sabine x (2006 paper):
%
%x = Qk T C D (6.4.5.7)  (C=1 for no absorption)
%T: average path length through crystallite (so (3/2)*radius for a sphere)
%
%
%Sabine x Shuqi:
%  double xp = NC::ncsquare(Nc * wl * F_hkl * l);
%
%Sabine x for spheres (1988 paper):
%
%x = Qk T D (middle of left column, p371)
%Qk = |F/V|^2*wl^2 / sintheta (top right column, p370)
%BUT NOTE: Qtheta = |F/V|^2*wl^3 / sin2theta ``on the glancing-angle scale''(top right column, p370) (FOR SECONDARY EXTINCTION????)
%
%371 notes for spheres T=D=(3/2)*radius
%
% So x = |F/V|^2*wl^2 / sintheta * (3radius/2)^2


%Nice formulation: "... the coherently scattering domain size, referred to here as the crystallite size for readability."

%Could we somehow use "Multiple Bragg reflection by a thick mosaic
%crystal. II. Simplified transport equation solved on a grid, Wuttke 2020" ?

%wilson pgs 41+42: 0.93, 0.94, 1.0 => O(1-6%) mistake if modelling a cubic
%domain as spherical. ??

%wilson eq 9-12, is how we can see that T = (3/4) * diameter = (3/2) * radius
%for a sphere. The "apparent particle size" is the volume average of the path
%length. Check also eq. 19 for how this could be used to take hkl anisotropy
%into account.

%Discuss also why we focus on Sabine + BC (because it is discussed in e.g. the
%BC paper that CR, Zach, etc. are incorrect).

%ARGH: Trying to "spherify" Sabine's El and Eb makes the result go FURTHER from
%BC, not closer. One solution is to look at low-x, and apply a correction to x
%in the sabine model, which makes the constant in the first order term of yp(x)
%~= 1-const*x match up. This turns out to be a factor of 19/15 ~= 1.26666,
%meaning that the block sizes of Sabine are essentially modified by
%sqrt(19/15)~=1.125 (very close to 9/8). The nice thing in practice here is how
%we would know that the two models have similar behaviour at low x.

%FIXME: Lorentz breakdown plot shows a breakdown of the updated-classic recipe,
%       which is missed in the standard cmprecipes plot, since it only occurs
%       for 45<theta<46. Solution: Rerun with the point at 45 replaced with
%       44.99 and 45.01 ? Or add those as extra points?


% FIXME: NOTICE THAT theta=90 and theta=pi are inconsistent. Once case is
% theta_bragg and one is theta_scatter=2*theta_bragg. But we are using pi as
% index on functions etc. to indicate backscattering. There is not direct easy
% fix, since theta in sintheta = lambda/2d is thetabragg, and theta=pi is
% theta_scatter. Solution: Use theta_bragg everywhere, but for convenience use
% pi as sub/super-script on functions to indicate 2*thetabragg=pi.


%OUTLOOK/ discussion: BC models used a lot, but the issues is by all means not
%settled (e.g Takagi's equations versus non-wave transfer equations). However,
%the aim of the present work is not to settle the field once and for all, but to
%ensure that the community has better recipes for applying one of the better
%developed models out there. (NB:
