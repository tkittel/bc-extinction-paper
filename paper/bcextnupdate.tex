%------------------------------------------------------------------------------
% Template file for the submission of articles to IUCr journals in LaTeX2e
% using the iucrjournals document class (file iucrjournals.cls)
% This work has been dedicated to the public domain
% License: CC0 1.0 Universal
% https://creativecommons.org/publicdomain/zero/1.0/
%------------------------------------------------------------------------------
% This template file and associated class and style files produce documents in
% a preprint style suitable for submission and review purposes.
% The iucrjournals.cls requires a small selection of packages from standard TeXLive
% distributions and contains a minimal set of macros to define content and apply
% formatting. BibTeX and iucr.bst should be used for references (using harvard.sty).
% If you wish to use additional packages, please reference them in this document and
% please only use packages included in standard TeXLive distributions in order to
% avoid compilation problems during the submission process.
%------------------------------------------------------------------------------

\documentclass{iucrjournals}

\usepackage{amssymb}
\usepackage[fleqn]{amsmath}
\usepackage{booktabs}
\usepackage[table]{xcolor}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{subcaption}
\usepackage{caption}
\usepackage[section]{placeins}

\DeclareMathOperator{\sinc}{sinc}
\newcommand{\deta}{{\mathrm{d}\eta}}
\newcommand{\order}{\mathcal{O}}
\newcommand{\labsec}[1]{\label{sec:#1}}
\newcommand{\refsecnumonly}[1]{\ref{sec:#1}}
\newcommand{\refsec}[1]{Section~\refsecnumonly{#1}}
\newcommand{\Refsec}[1]{Section~\refsecnumonly{#1}}%same as \refsec, but always capitalised!
\newcommand{\reftwosections}[2]{Sections~\refsecnumonly{#1} and \refsecnumonly{#2}}
\newcommand{\refthreesections}[3]{Sections~\refsecnumonly{#1}, \refsecnumonly{#2}, and \refsecnumonly{#3}}
\newcommand{\refsectionrange}[2]{Sections~\refsecnumonly{#1}--\refsecnumonly{#2}}
\newcommand{\labappendix}[1]{\label{app:#1}}
\newcommand{\refappendixnumonly}[1]{\ref{app:#1}}
\newcommand{\refappendix}[1]{Appendix~\refappendixnumonly{#1}}
\newcommand{\refalgnumonly}[1]{\ref{alg:#1}}
\newcommand{\refalg}[1]{Algorithm~\refalgnumonly{#1}}
\newcommand{\refalgrange}[2]{Algorithms~\refalgnumonly{#1}--\refalgnumonly{#2}}
\newcommand{\labfig}[1]{\label{fig:#1}}
\newcommand{\reffignumonly}[1]{\ref{fig:#1}}
\newcommand{\reffig}[1]{Figure~\reffignumonly{#1}}
\newcommand{\Reffig}[1]{Figure~\reffignumonly{#1}}%same as \reffig, but always capitalised!
\newcommand{\refthreefigs}[3]{Figures~\reffignumonly{#1}, \reffignumonly{#2}, and \reffignumonly{#3}}
\newcommand{\reftwofigures}[2]{Figures~\reffignumonly{#1} and \reffignumonly{#2}}
\newcommand{\reffigrange}[2]{Figures~\reffignumonly{#1}--\reffignumonly{#2}}
\newcommand{\refsubfigfrommaincaption}[1]{Figure~\protect\subref{fig:#1}}
\newcommand{\Refsubfigfrommaincaption}[1]{Figure~\protect\subref{fig:#1}}%same, but always capitalized!
\newcommand{\reftwosubfigsfrommaincaption}[2]{Figures~\protect\subref{fig:#1} and ~\protect\subref{fig:#2}}
\newcommand{\labtab}[1]{\label{tab:#1}}
\newcommand{\reftabnumonly}[1]{\ref{tab:#1}}
\newcommand{\reftab}[1]{Table~\reftabnumonly{#1}}
\newcommand{\Reftab}[1]{Table~\reftabnumonly{#1}}%same as \reftab, but always capitalised!
\newcommand{\reftabrange}[2]{Tables~\reftabnumonly{#1}--\reftabnumonly{#2}}
\newcommand{\labeqn}[1]{\label{eqn:#1}}
\newcommand{\refeqnnumonly}[1]{\ref{eqn:#1}}
\newcommand{\refeqn}[1]{Eq.~\refeqnnumonly{#1}}
\newcommand{\refeqnrange}[2]{Eqs.~\refeqnnumonly{#1}--\refeqnnumonly{#2}}
\newcommand{\Refeqn}[1]{Eq.~\refeqnnumonly{#1}}%same as \refeqn, but always capitalised!
\newcommand{\reftwoeqns}[2]{Eqs.~\refeqnnumonly{#1} and \refeqnnumonly{#2}}

\title{Revisiting Becker-Coppens (1974): Updated Recipes for Estimating Extinction Factors in Spherical Crystallites}

% Authors and affiliations (uses the standard authblk package):
% Author affiliations are indicated by lowercase letters in square brackets in the \author macro.
% Affiliations (referenced by the lowercase letters in square brackets) are listed after all the authors have been defined.
% The email addresses of corresponding/contact authors can be included using:
% \IUCrCemaillink{corrauthor@org.org}
% Other co-author email addresses can be included using:
% \IUCrEmaillink{coauthor@org.org}
% ORCiDs can be included using:
% \IUCrOrcidlink{xxxx-xxxx-xxxx-xxxx}
% Author footnotes can be included using:
% \IUCrAufn{Text...}
% and to apply the same footnote to another author use:
% \IUCrAufn[1]{}
% where the number in square brackets refers to the numerical order of the
% previously defined footnote.
% For example:
% \author[a]{Anne Author\IUCrCemaillink{corrauthor@org.org}\IUCrOrcidlink{xxxx-xxxx-xxxx-xxxx}}

\author[a]{Thomas Kittelmann\IUCrCemaillink{thomas.kittelmann@ess.eu}\IUCrOrcidlink{0000-0002-7396-4922}}%
\author[b]{Douglas D. DiJulio\IUCrEmaillink{douglas.dijulio@ess.eu}\IUCrOrcidlink{0000-0002-8025-200X}}%
\author[b]{Shuqi Xu\IUCrEmaillink{shuqi.xu@ess.eu}\IUCrOrcidlink{0000-0002-9615-7533}}%
\author[b]{J. I. M\'arquez Dami\'an\IUCrEmaillink{jose.marquez@ess.eu}\IUCrOrcidlink{0000-0002-9611-914X}}%
\affil[a]{ESS Data Management and Scientific Computing Division, Lyngby, Denmark}
\affil[b]{European Spallation Source ERIC, Lund, Sweden}

\begin{document}
\maketitle

\begin{synopsis}
A technical update of the Becker-Coppens (1974) recipe for estimating extinction
factors in spherical crystallites or grains is provided, addressing limitations
in scenarios with higher Bragg angles or stronger extinction effects, as well as
providing improved precision in general. The work utilises modern computing
capabilities and improved recipes are provided for easy community adoption.
\end{synopsis}

\begin{abstract}
A technical correction and update of the widely used recipes by Becker-Coppens
(1974), for the estimation of primary and secondary extinction factors in
perfect spherical crystallites or grains, is presented. In the original work,
these extinction factors were evaluated numerically from a complicated integral,
and simplified analytical approximations to these evaluations constituted the
provided recipes. However, these original recipes are plagued by issues of
numerical precision in general, and even suffer from complete numerical
break-down in case of strong extinction effects, especially in backwards
scattering. Using modern computing capabilities, the numerical evaluations of
the integrals are revisited, and improved recipes are provided with consistent
precision guarantees for all Bragg angles and levels of extinction. The new
recipes are provided both in a ``standard'' version, believed to be of suitable
precision for any actual analysis of diffraction or transmission data, and a
``luxury'' reference version with even higher precision.

The performance of the new recipes is naturally compared to those of the
original work, and in order to facilitate easy adoption by the community,
reference implementations are provided for \texttt{C}, \texttt{C++}, and
\texttt{Python} languages.
\end{abstract}

\keywords{Extinction; Bragg diffraction; Neutron scattering; X-ray diffraction; }

\section{The Becker-Coppens recipes for extinction}

Extinction in diffraction refers to the phenomenon where the intensity of a
diffracted flux is effectively reduced due to various effects like multiple
scattering or wave interference between incoming and scattered waves. More
specifically, the magnitude of this effect is characterized by the extinction
factor, $y$, which is defined as the ratio between the diffracted flux predicted
by the kinematical model ($I_\text{kin}$) with no multiple scattering or other
dynamic effects, and the diffracted flux which is actually observed once these
effects are accounted for ($I_\text{obs}$):
\begin{align}
y \equiv \frac{I_\text{obs}}{I_\text{kin}}
\end{align}
The extinction factor $y$ is a real number in the unit interval, which in
general will depend on both the state of the particle undergoing diffraction as
well as of the details of the material in which the process takes place.  In all
cases, $y$ is expressed as a function of both the Bragg angle $\theta$ as well
as a parameter $x$ which captures the general strength of the extinction
effect. The exact definition of $x$ depends to some degree on the specific
model, but in general it will be proportional to $|DF\lambda/V|^2$ where $F$ is
the crystalline structure factor of the reflection in question, $V$ the volume
of the crystal unit cell, $\lambda$ the wavelength of the particle being
scattered, and $D$ some relevant length scale connected to the path length of
the probe through the coherent domains, crystallites, or grains in which the
diffraction is taking place. In the case where the extinction is governed by a
mosaic distribution, the definition of $x$ will usually also depend on
parameters of this mosaic distribution, such as in particular its width.

Several models for $y$ has been proposed over the years. Early work by
Zachariasen \citeyear{zachariasen1967} was improved upon by other authors like
Cooper-Rouse \citeyear{cooper1970extinction}, Becker-Coppens \citeyear{BC1974},
and Sabine \citeyear{sabine2006flow}. These models have been primarily developed
under the context of x-ray and neutron diffraction measurements. More recently,
however, advancements in neutron transmission applications, particularly for
Bragg-edge imaging and nuclear data evaluation, have renewed interest in
extinction
effects~\cite{sato2011,dessieux2018single2poly,dijulio2020berylrefl,Xu2025extnbetransm}.
Given this renewed intereset and advances in modern computing capalities,
revisiting these models is warrented. As the Becker-Coppens model is arguably
the more highly referenced and utilised work, it was naturally considered by the
authors when working on implementing extinction models in \texttt{NCrystal}
\cite{ncrystalmain,ncrystalelastic}, in order to facilitate the modelling and
analysis of Bragg edge transmission and diffraction data. Unfortunately, in case
of stronger extinction effects, this model appears to suffer from an obvious
breakdown at large Bragg angles, as can be seen in
\reffig{ypbreakdownorigrecipe}.
\begin{figure}[tp] %
\begin{center}
\includegraphics[width=0.6\textwidth]{generated_with_ncrystal/ncrystal_bc_yp_breakdown_be.pdf}
\end{center}
\caption{Bragg diffraction cross sections in a beryllium powder as calculated by
  \texttt{NCrystal}, including primary extinction effects for idealised spherical
  crystallites of various radii as predicted by the Becker-Coppens model using
  both the recipes from this (BC2025) and the original work (BC1974).}
\labfig{ypbreakdownorigrecipe}
\end{figure}

As will be shown in the present work, this breakdown does not constitute a
fundamental problem with the Becker-Coppens model, but is merely a result
of flawed numerical evaluations entering into the recipes (algorithms) provided
in that work. It is the aim of the present work to provide updated recipes which
does not change the underlying theory or assumptions going into the
Becker-Coppens model, but merely ensures a consistent and precise numerical
evaluation of it. These new recipes for evaluation of $y$ will be referred to as
BC2025 in the present work, while the recipes from the original work will be
referred to as BC1974. It will be seen that the $y$ values provided by the
BC2025 recipes do not drastically disagree agree with those of the BC1974
recipes in the regions of intermediate extinction levels, where the model
arguably has seen most usage and validation over the years in X-ray or neutron
diffraction. For reference the effect of using the BC2025 recipe is also shown
in \reffig{ypbreakdownorigrecipe}, and is seen to not suffer from the same
breakdown as the original recipe.

In the work by Becker-Coppens \cite{BC1974}, four specific models and recipes
are provided for the determination of $y$: one for primary extinction, and three
for secondary extinction, where the three latter models only differ in the
choice of distribution used to describe the mosaic spread of coherent domains
within the grains: Gaussian, Lorentzian, or Fresnellian.

\subsection{Mathematical formulation of the models}
Readers are referred to the original work \cite{BC1974} for the detailed
description and background of the models, while the present work will focus
exclusively on the mathematical expressions from the models which must be
evaluated. More specifically, the Becker-Coppens models all require evaluation
of the following integral, in order to determine the extinction parameter $y$ as
a function of $x$ and $\theta$:
\begin{align}
  y_M(\theta,x) = \frac{6c_M}{4\pi}\int_{0}^{\infty} f_M(\eta)\varphi\left(\theta,c_Mxf_M(\eta)\right)\deta
  \labeqn{yintegral}
\end{align}
Here, the subscript $M$ indicates the model, and can either be $P$ (primary),
$G$ (secondary Gaussian), $L$ (secondary Lorentzian), or $F$ (secondary
Fresnellian). The variable $c_M$ is a constant, with model dependent values of
$c_P=c_G=c_F=1$ and $c_L=4/3$. It should be noted that the exact definition of
$x$ is given in the original work, and also depends on the model.

The function $\varphi(\theta,s)$ is an extinction correction function given by:
\begin{align}
  \varphi(\theta,s) = \varphi^0(s) + \sin^{3/2}\theta\left\{\varphi^\pi\left(s\sqrt{\sin\theta}\right)-\varphi^0\left(s\sqrt{\sin\theta}\right)\right\}
  \labeqn{phi}
\end{align}
Here $\varphi^0(s)$ and $\varphi^\pi(s)$ are the limiting functions for
$\theta=0$ and $\theta=\pi/2$ respectively,\footnote{To be concise, the
superscript $\pi$ will as in the original work in general be used on variables
related to complete back-scattering throughout the present work, even though the
Bragg angle associated with such scatterings is strictly speaking $\pi/2$.} and
are given as:\footnote{Note that the expression for $\varphi^\pi(s)$ in Eq. 32
of \cite{BC1974} has a mistake, where a factor of $1/2$ should be moved from the
second to the third term.}
\begin{align}
  \varphi^0(s) &= \frac{3}{64s^3}\left\{8s^2+4s\exp(-4s)-(1-\exp(-4s))\right\}\\
  \varphi^\pi(s) &= \frac{3}{4s^3}\left\{s^2-s+\frac{1}{2}\log(1+2s)\right\}
  \labeqn{phizeroandphipi}
\end{align}
All the $\varphi$--functions are, as illustrated in \reffig{phiofs},
monotonically decreasing functions of $s$ for $s\ge0$, attaining a value of $1$
at $s=0$ and a limiting value of $0$ as $s\to\infty$.
\begin{figure}[tp] %
\begin{center}
\includegraphics[width=0.6\textwidth]{generated/phi_of_s.pdf}
\end{center}
\caption{The $\varphi^0(s)$ and $\varphi^0(s)$ functions from \refeqn{phizeroandphipi}.}
\labfig{phiofs}
\end{figure}

The $f_M(\eta)$ functions are essentially unit-less functions capturing the
shape of the Bragg diffraction cross section as a function of deviation from the
nominal direction, normalised so $f_M(0)=1$. The difference in extinction levels
predicted by the different Becker-Coppens models is essentially due to the
different shapes of these $f_M(\eta)$ functions. For the secondary extinction
models, the shape of $f_M(\eta)$ is essentially given by the mosaic distribution
of coherent crystalline domains within larger grains, while for primary
extinction it is due to the intrinsic response of a crystalline domain which
contains a perfect and unbroken crystal structure, but is of finite
size. Additionally, the exact final form of the $f_M(\eta)$ functions is based
on the assumption of spherical geometries for the crystallites or grains. The
exact form of these functions, plotted in \reffig{fofeta}, is as follows:
\begin{figure}[tp] %
\begin{center}
\includegraphics[width=0.6\textwidth]{generated/f_of_eta.pdf}
\end{center}
\caption{The $f_M(\eta)$ functions from \refeqnrange{fofeta:P:first}{fofeta:F:last}.}
\labfig{fofeta}
\end{figure}
\begin{subequations}
  \begin{align}
    f_P(\eta) &= \frac{\eta^2-\eta\sin(2\eta)+sin^2(\eta)}{\eta^4}\labeqn{fofeta:P:first}\\
    f_G(\eta) &= \exp\left\{-\frac{9}{16\pi}\eta^2\right\}\labeqn{fofeta:G}\\
    f_L(\eta) &= \frac{1}{1+\eta^2}\labeqn{fofeta:L}\\
    f_F(\eta) &= \frac{ \sin^2(3\eta/4) } { (3\eta/4)^2 }\labeqn{fofeta:F:last}
  \end{align}
\end{subequations}

The task of providing updated recipes for the evaluation of \refeqn{yintegral}
is twofold. Firstly, a reference dataset must be created by numerically
evaluating the integral to high precision on a sufficiently large number of
$(x,\theta)$ points. This is the subject of \refsec{refeval}.  Secondly, a
recipe must be found and tuned on this dataset in order to reproduce its values
with sufficient precision. As such recipes would ultimately be used in data
modelling or analysis, it should be possible to implement them in software with
a reasonably low cost of evaluation. Fortunately, computing hardware has evolved
tremendously since the original recipes by Becker-Coppens were developed, and
some additional degree of complexity in the updated recipes is therefore
allowed.

One important issue in all of this is the definition of precision of a given
estimate of $y$, compared with a reference value $y_\text{ref}$. The naive
definition $\text{precision}(y)\equiv|y-y_\text{ref}|/y_\text{ref}$ is not
particularly useful at low levels of extinction where $y$ values are always
close to $1$, and where the quantity distinguishing different models might
arguably be $1-y$ rather than $y$ itself. To ensure that the developed recipes
for determining $y$ are useful and trustworthy in all scenarios, a more
stringent definition of precision will thus be used throughout this work:
\begin{align}
  \text{precision}(y) \equiv
  \frac{|y-y_\text{ref}|}{\min(y_\text{ref},1-y_\text{ref})}
  \labeqn{yprecisiondef}
\end{align}
Naturally, for this definition to be useful in practice, this requires
$y_\text{ref}$ itself to be available in a precision significantly higher than
the precision on $y$.

\section{Evaluating the integrals}\labsec{refeval}
The integral in \refeqn{yintegral} was evaluated numerically, in order to
provide a reference data set with which to develop, tune and benchmark updated
recipes for $y_M(x,\theta)$. The most precise of the recipes developed in
\refsec{newrecipes} guarantees a precision level according to
\refeqn{yprecisiondef} of $10^{-6}$ or better, so consequently the numerical
evaluation of the integral is done to a precision level of $10^{-8}$ or
better. In most cases, it will be a lot better, due to the consistent usage of
very conservative error estimates.

The behaviour of the integrand of \refeqn{yintegral} is illustrated in
\reffig{integrandfcts}, and is seen to carry over features from the equivalent
$f_M(\eta)$ functions in \refeqnrange{fofeta:P:first}{fofeta:F:last}. In
particular two of these features are challenging from the point of view of
numerical quadrature: Slow decay ($\sim1/\eta^2$) and oscillatory behaviour. Only
the Gaussian model ($M=G$) exhibits neither of these, and is therefore
relatively straight-forward to evaluate via numerical quadrature.
\begin{figure}[tp]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/integrand_gPgGgLgF_of_eta_g.pdf}
        \caption{}\labfig{integrandfcts:subfig:gPGLF}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/integrand_gP_of_eta_g.pdf}
        \caption{}\labfig{integrandfcts:subfig:gP}
    \end{subfigure}
    \caption{\Refsubfigfrommaincaption{integrandfcts:subfig:gPGLF} shows the
      integrand of \refeqn{yintegral} for the four different models at
      $x=1$ and $\theta=45^\circ$. \Refsubfigfrommaincaption{integrandfcts:subfig:gP}
      shows the integrand for the primary extinction model ($M=P$) at various
      $(x,\theta)$ points.}
    \labfig{integrandfcts}
\end{figure}

In order to reduce the concern of floating point precision, all work described
in this section is carried out in \texttt{Python} using the arbitrary-precision
floating point arithmetic provided by the \texttt{mpmath} package \cite{mpmath},
configured to keep at least 150 digits of precision in all
operations. Additionally, the implementation of the $\varphi^0(s)$,
$\varphi^\pi(s)$, and $f_M(\eta)$ functions were all implemented in a
numerically stable manner. Specifically, all special functions are evaluated
with special \texttt{mpmath} routines where appropriate (including the $\sinc$
function for $f_F(\eta)$), and the $f_P(\eta)$, $\varphi^0(s)$, and
$\varphi^\pi(s)$ functions are evaluated using Taylor expansions of at least 50
orders when their arguments are small, since their direct mathematical forms are
otherwise numerically unstable in that region. The Taylor expansions are found
with the help of the \texttt{SageMath} \cite{sagemath} mathematics software
system.\footnote{For reference, all the code and datasets associated with the
present work are provided in \cite{bc2025materials}, and can be run completely
with an installation of purely Open Source tools. The main software used is
\texttt{mpmath}, \texttt{SageMath}, \texttt{NumPy} \cite{numpy}, and
\texttt{matplotlib} \cite{matplotlib}.}

The actual numerical quadrature evaluation of the integral is based on
Gauss-Legendre quadrature, and is performed over a large number of sub-intervals
of $\eta$ values in order to ensure the highest precision of results. This
approach works well for reasonable $x$ values, say $0.001<x<1000$, with the
exact limits depending on the model $M$. For practical reasons, we shall limit
$x\le1000$, and simply provide a reasonable extrapolation to higher $x$ values
in the recipes developed in \refsec{newrecipes}. This is considered adequate
since $x>1000$ is associated with such hefty levels of extinction that almost no
scattering cross section remains anyway. Also note in this regard that the
original Becker-Coppens recipes only provided numerical reference values for
$0.1\le{x}\le30$. For low $x$ values it is fortunately the case that it has been
possible to find Taylor series approximations to the exact result near $x=0$
using \texttt{SageMath}. This not only provides computationally inexpensive
high-precision results near $x=0$, but also serves as a valuable verification of
the numerical quadrature results in the overlapping region where both methods
can be used. For $M=P$, $G$, $L$, and $F$ respectively, 30, 60, 60, and 20
orders are used in the Taylor expansions. The sum of the absolute value of the
last three terms in the expansions are used as a very conservative estimate on the
error of these results. For reference, the first five orders of the Taylor
expansions are shown here:
\begin{align}
  y_P(x,\theta)\approx&\,  1 - \tfrac{33}{35}x
+ \tfrac{2369}{5775}(\sin^{5/2}\theta + 2)x^2
 - \tfrac{54120476}{91216125}(1+2\sin^3\theta)x^3\labeqn{ytaylor:P:first}\\\nonumber
&+ \tfrac{350769113251}{1924903480500}(2+13\sin^{7/2}\theta)x^4
 - \tfrac{276478446363113}{2846107289025000}(2+43\sin^4\theta)x^5\\
  y_G(x,\theta)\approx&\,  1 - \tfrac{3\sqrt{2}}{4}x
  + \tfrac{4\sqrt{3}}{15}(2+\sin^{5/2}\theta)x^2
- \tfrac{2}{3}(1+2\sin^3\theta)x^3\labeqn{ytaylor:G}\\\nonumber
 &+ \tfrac{16\sqrt{5}}{175}(2+13\sin^{7/2}\theta)x^4
 - \tfrac{2\sqrt{6}}{45}(2+43\sin^4\theta)x^5\\
  y_L(x,\theta)\approx&\,  1-x
  + \tfrac{8}{15}(2+\sin^{5/2}\theta)x^2
- \tfrac{80}{81}(1+2\sin^3\theta)x^3\labeqn{ytaylor:L}\\\nonumber
&+ \tfrac{32}{81}(2+13\sin^{7/2}\theta)x^4
- \tfrac{112}{405}(2+43\sin^4\theta)x^5\\
  y_F(x,\theta)\approx&\, 1 - x
  + \tfrac{11}{25}(2+\sin^{5/2}\theta) x^2
  - \tfrac{604}{945}(1+2\sin^3\theta)x^3 \labeqn{ytaylor:F:last}\\\nonumber
  &+ \tfrac{15619}{79380}(2+13\sin^{7/2}\theta)x^4
   - \tfrac{655177}{6237000}(2+43\sin^4\theta)x^5
\end{align}

The next issue is that it is obviously not practically possible to perform the
numerical quadrature over $\eta$ all the way to infinity. For $M=G$ the
contributions decrease so strongly with $\eta$, that there is never any
significant contribution to the integral for $\eta>1000$, and accordingly
numerical quadrature to this value is always sufficient to ensure the desired
precision on $y$ values. For $M=L$, the integrand is sufficiently simple that it
was possible to find a Taylor series approximation for the tail of the integral
with \texttt{SageMath}:
\begin{align}
\int_{a}^{\infty} f_L(\eta)\varphi\left(\theta,xf_L(\eta)\right)\deta \approx
\frac{1}{a} - \frac{2k}{3a^2}
+\left(
\frac{128 + 64{\sin^{5/2}\theta}}{225}k^2
-\frac{1}{3}
\right)\frac{1}{a^3} + \ldots
\labeqn{tailintegralL}
\end{align}
Here $k=x/a$ was introduced, and it was required that $a>\max(x,1)$, implying
$0\le{k}<1$. For brevity, only the first few terms are shown in
\refeqn{tailintegralL}, but in the actual implementation, terms up to $1/a^{20}$
are included. Again, the error bound is conservatively estimated as the sum of
the absolute values of the last three terms (of order $1/a^{18}$, $1/a^{19}$,
and $1/a^{20}$). Thus, in practice the error bound will be of order $1/a^{18}$,
requiring actual numerical quadrature only over $[0,a]$ for very modest values
of $a$. For $M=P$ the integrand is more challenging due to the presence of
trigonometric functions. Firstly, the desire for high precision results implies
that the $\eta$ ranges passed into the numerical quadrature machinery will be
limited to always be shorter than the period of these trigonometric functions,
with a resulting large increase in the number of times the Gauss-Legendre
quadrature algorithm must be applied. Secondly, the increased complexity of the
integrand means that unlike the case of $M=L$, it was not possible to find a
direct Taylor series approximation for the tail integral from
$[a,\infty]$. However, as is shown in \refappendix{yptailintegration}, by
bounding $f_P(\eta)$ by simpler non-oscillatory functions, it was possible to
find a somewhat looser but still useful bound on the tail contribution in this
case as well:
\begin{align}
\int_{a}^{\infty} f_P(\eta)\varphi\left(\theta,xf_P(\eta)\right)\deta \approx
\frac{1}{a}  - \frac{x}{2a^3 } \pm\left(\frac{1}{2a^2 }+\frac{2}{a^3}\right),\quad{}a>10,\,a>x>0
\labeqn{tailintegralP}
\end{align}
Here the indicated error is a conservative upper bound on the error, and
unfortunately the slower convergence of just $\order(1/a^2)$ implies that actual
numerical quadrature is required over $[0,a]$ for rather significant values of
$a$, but fortunately not impossible on modern hardware.

Finally, the case of $M=F$ poses a somewhat different challenge, due to the fact
that it was not possible to find sufficiently simple functions bounding
$f_F(\eta)$ tight enough to provide a useful estimate of the contribution of the
tail of the integral. Since the integrand additionally decays only as $1/\eta^2$
and suffers from the same oscillations as $f_P(\eta)$, it is impractical to
simply integrate over $[0,a]$ for a sufficiently large $a$. Thus, a different
approach was used in this case, instead defining the $n$th contribution as the
integral over the interval $[(4\pi/3)n,(4\pi/3)(n+1)]$, finding the
contributions up to at least $n=1000$ and then use Richardson's extrapolation
\cite{richardson1911ix} via \texttt{mpmath}'s \texttt{nsum} function to include
the contributions up to $n=\infty$.

The resulting curves of $y_M(x,\theta)$ reference values are shown in
\reffig{yrefeval}, and for future reference \refappendix{yreftables} contains a
selection of such values in \reftabrange{yrefvals:P:first}{yrefvals:F:last}.
\begin{figure}[tp]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/yrefeval_primary.pdf}
        \caption{}\labfig{yrefeval:subfig:primary}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/yrefeval_bands.pdf}
        \caption{}\labfig{yrefeval:subfig:bands}
    \end{subfigure}
    \caption{\Refsubfigfrommaincaption{yrefeval:subfig:primary} shows the
      $y_P(x,\theta)$ curves resulting from the high precision evaluation of
      \refeqn{yintegral}, with each colour denoting a specific value of $\theta$
      as indicated. \Refsubfigfrommaincaption{yrefeval:subfig:bands}
      shows the bands of $y_M(x,\theta)$ values covered by all four models, as
      $\theta$ is varied from $0^\circ$ to  $90^\circ$.}
    \labfig{yrefeval}
\end{figure}

\section{Original recipes}\labsec{origrecipes}
In the original work, the authors provide only few details of how
\refeqn{yintegral} has been evaluated, leaving the impression that integrals
have been evaluated with a single application of a Gaussian quadrature algorithm
over a finite interval. In light of the work discussed in \refsec{refeval}, this
appears to have been inadequate -- but, given the computational power available
when the work was carried out more than half a century ago, certainly
understandable.

The recipes for estimating $y_M(\theta,x)$ provided in the original work are
relatively simple, all using the same general form:
\begin{align}
  y_M(\theta,x)=\frac{1}{\sqrt{1+t_Mx+\frac{A_M(\theta)}{1+x{\cdot}B_M(\theta)}x^2}}
  \labeqn{yfitfctorig}
\end{align}
Here $t_M$ are model dependent constants governing the small-$x$ behaviour, given by:
\begin{align}
  t_P=t_L=t_F=2,\quad{}t_G=2.12
  \labeqn{yfitfctlowxconstants}
\end{align}
The $\theta$ dependency is captured in the model dependent functions
$A_M(\theta)$ and $B_M(\theta)$:
\begin{align}
  \labeqn{yfitfct:AP:first}
  A_P(\theta) &= 0.20 + 0.45\cos(2\theta) \\
  B_P(\theta) &= 0.22 - 0.12\cdot(0.5-\cos(2\theta))^2\\
  A_G(\theta) &= 0.58+0.48\cos(2\theta)+0.24 \cos^2(2\theta)\\
  B_G(\theta) &= 0.02-0.025\cos(2\theta)\\
  A_L(\theta) &= 0.025+0.285\cos(2\theta)\\
  B_L(\theta) &=
  \begin{cases}
    0.15-0.2\cdot(0.75-\cos(2\theta))^2\quad&\text{when }\cos(2\theta) \ge 0\\
    -0.45\cos(2\theta)\quad&\text{when }\cos(2\theta) {<} 0
  \end{cases}\labeqn{yfitfct:BL}\\
  A_F(\theta) &= 0.48 + 0.6\cos(2\theta)\\
  B_F(\theta) &= 0.20-0.6\cdot(0.2-\cos(2\theta))^2
  \labeqn{yfitfct:BF:last}
\end{align}
These recipes are certainly very compact and easily implementable. However, they
unfortunately suffer greatly from precision issues, and even completely break
down in some cases where the argument of the square root in \refeqn{yfitfctorig}
become negative. This happens for some $\theta$ values only, and primarily at
high values of $x$. In addition to this, the recipe for $M=P$ also suffers from
an issue at small $x$. The first order Taylor expansion of \refeqn{yfitfctorig}
is $1-t_M/2$ which can be compared with the first order terms in
\refeqnrange{ytaylor:P:first}{ytaylor:F:last}. Doing so determines the correct
$t_M$-values to be $t_L=t_F=2$, $t_G=3/\sqrt{2}\approx2.12$, and
$t_P=66/35\approx1.89$. Thus, the $t_P$ values in \refeqn{yfitfctlowxconstants}
are incorrect, and it is easy to see that this gives a limiting (im)precision at
small $x$-values of $2/33\approx6.1\%$ (cf.~\refeqn{yprecisiondef}).  For the
case $M=L$, there is additionally a small but noteworthy issue of an
incontinuity at $\cos(2\theta)=0$ in \refeqn{yfitfct:BL}. The coefficients at
first glance looks like they were deliberately chosen to avoid this, since
$0.15-2\cdot0.75=0$, but the factor of $0.75$ actually enters in quadrature.

For a more complete view of the applicability of the BC1974 recipes,
\reffig{prec2dplot} shows the precision for the four models as a function of
both $x$ and $\theta$. Although the exact contours of precision levels have
non-trivial shapes, it is clear that in general the recipes can only be said to
adequately describe the reference values at moderate or low values of $x$, with
the upper limit of applicability lying from around $x=1$ to $x=10$, depending on
the desired precision, on the model and in particular also on $\theta$
values. In general, there is also a tendency for the recipes to have worse
performance for more backwards scattering, and as expected the small $x$ behaviour
when $M=P$ has the worst precision of all the models, consistent with the
predicted value of $6.1\%$.

The origin of the limitations of the original recipes is likely
two-fold. Firstly, the reference set of $y$ values on which the formulae were
tuned was somewhat limited as can be seen from the dashed boxes in
\reffig{prec2dplot}, covering only $x$ values from 0.1 to 30 and $\sin\theta$
values from 0.05 ($\theta\approx2.9^\circ$) to 0.9 ($\theta\approx64^\circ$). In
the case of $M=P$ the authors themselves noted the bad performance of their
model at $\sin\theta=0.9$, so a maximum value of 0.8 ($\theta\approx53^\circ$)
was used in this case. One can only speculate as to the reason for the limited
range of training data, but it seems reasonable to assume that it was a result
of most relevant experimental data analysis happening at lower angles of
$\theta$ in combination with the limited computing resources available at the
time. The other reason for the limitations of the recipes is, as already
mentioned, the inaccurate evaluation of \refeqn{yintegral} to provide reference
values of $y_M$. For all models except $M=F$, these reference values were
actually included as tables 1, 3, and 4 in the original work allowing a direct
gauging of their accuracy. These accuracies are shown in
\reftabrange{origtable1precision}{origtable4precision},\footnote{The original
tables were digitised using the \texttt{Tesseract} OCR software and manually
verified. Any odd entries, such as for $(x,\sin\theta)$=$(4,0.2)$ in
\reftab{origtable1precision} or $(x,\sin\theta)$=$(0.6,0.5)$ in
\reftab{origtable3precision}, were manually double-checked and are likely a sign
of transcription errors in the tables of the original work.} and clearly there
are significant systematic deviations in particular at higher values of $x$, to
some degree consistent with what is seen in \reffig{prec2dplot}.

\begin{table}[tp]
\caption{Precision of the $y_P$ reference values in Table 1 of
  \protect\cite{BC1974} when compared with the reference values of the present
  work. The precision is defined as per \refeqn{yprecisiondef}, but including
  the sign of $y-y_\text{ref}$. Values are shown as percentages (\%), and cell
  colours are used to more easily visualise larger imprecisions and trends with
  negative (positive) values shown in blue (red).}
\smallskip
\tiny
\begin{center}
\input{generated/table1_diff.tex}
\end{center}
\labtab{origtable1precision}
\end{table}

\begin{table}[tp]
\caption{Precision of the $y_G$ reference values in Table 3 of
  \protect\cite{BC1974}, shown in same manner as for
  \reftab{origtable1precision}.}
\smallskip
\tiny
\begin{center}
\input{generated/table3_diff.tex}
\end{center}
\labtab{origtable3precision}
\end{table}

\begin{table}[tp]
\caption{Precision of the $y_L$ reference values in Table 4 of
  \protect\cite{BC1974}, shown in same manner as for \reftab{origtable1precision}.}
\smallskip
\tiny
\begin{center}
\input{generated/table4_diff.tex}
\end{center}
\labtab{origtable4precision}
\end{table}

\begin{figure}[tp]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/prec_classicrecipe_2d_primary.pdf}
        \caption{$M=P$}\labfig{prec2dplot:subfig:P}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/prec_classicrecipe_2d_scndgauss.pdf}
        \caption{$M=G$}\labfig{prec2dplot:subfig:G}
    \end{subfigure}
    \vspace{0.5cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/prec_classicrecipe_2d_scndlorentz.pdf}
        \caption{$M=L$}\labfig{prec2dplot:subfig:L}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/prec_classicrecipe_2d_scndfresnel.pdf}
        \caption{$M=F$}\labfig{prec2dplot:subfig:F}
    \end{subfigure}
    \caption{Precision as per \refeqn{yprecisiondef} of the $y_M(x,\theta)$
      values provided by the recipes in
      \refeqnrange{yfitfctorig}{yfitfct:BF:last} when compared with the high
      precision ``luxury'' recipes of the present work to be presented in
      \refsec{newrecipes}.  Boxes with white and black dashes indicate the
      limited range of the reference values used to tune the recipes in the
      original work.}  \labfig{prec2dplot}
\end{figure}

\section{Developing improved recipes}\labsec{newrecipes}
With the availability of reliable high-precision reference datasets of
$y_M(x,\theta)$ values from the work discussed in \refsec{refeval}, the next
step is then to provide a set of improved recipes tuned to this data -- in order
to replace the original ones by Becker-Coppens discussed in
\refsec{origrecipes}. For the benefit of future reference work, two version of
the recipes will be provided: ``standard'' versions, providing $y$ estimates
with errors guaranteed to be less than
$10^{-3}\cdot\min(y_\text{ref},1-y_\text{ref})$ for all $x\le1000$ and
``luxury'' versions where the error guarantee is tightened to be less than
$10^{-6}\cdot\min(y_\text{ref},1-y_\text{ref})$. For $x>1000$, no strict error
guarantee is given, but the recipes should still be reasonable and well behaved.

In order to develop these recipes, a crucial observation is that one does not
strictly need to parameterise $y_M(x,\theta)$ as a two-dimensional function like
was done in the original work. This is because it follows from
\reftwoeqns{yintegral}{phi} and the usual rules of integration that:
\begin{align}
  y_M(\theta,x) = y^0_M(x) + {\sin^{3/2}\theta}\,y^\Delta_M(x\sqrt{\sin\theta})
  \labeqn{yintegralbyy0ydelta}
\end{align}
with the following definitions:
\begin{align}
y^0_M(x)&\equiv y_M(x,0)\\
y^\pi_M(x)&\equiv y_M(x,\pi/2)\\
y^\Delta_M(x)&\equiv y_M^\pi(x)-y_M^0(x)
  \labeqn{y0pideltadef}
\end{align}
Thus, the recipe simply has to contain parameterisations of the two
one-dimensional functions $y^0_M(x)$ and $y^\Delta_M(x)$, and then implement the
$\theta$ dependency via \refeqn{yintegralbyy0ydelta}. In this respect it is
interesting to note that in most envisioned use-cases, it is $\sin\theta$ rather
than $\theta$ which is more cheaply calculated, due to its appearance in the
formula for the Bragg condition. This therefore fits nicely with
\refeqn{yintegralbyy0ydelta}, where only a single additional square root must be
evaluated rather than a more expensive trigonometric function. Thus, our recipes
will in practice be taking $\sin\theta$ rather than $\theta$ as argument.

In terms of the $x$-dependency of $y^0_M(x)$ and $y^\Delta_M(x)$, three
different regimes shall be considered. Firstly, the regime at very low
$x$-values is natural to simply evaluate via Taylor expansions. For this
purpose, it was chosen to use 5th order expansions for both $y^0_M(x)$ and
$y^\Delta_M(x)$, coefficients of which can trivially be determined from
\refeqnrange{ytaylor:P:first}{ytaylor:F:last}. The standard recipes will use the
Taylor expansions when $x<0.1$, while the luxury recipes will do so only when
$x<0.02$.

At the other extreme, $x$ values higher than $1000$ will as previously mentioned
simply be handled by extrapolation. As shown in \reffig{highx}, a simple power
law $y_M(x){\sim}x^{-p}$ appears to describe the curves well for $x\gtrsim100$,
so while the new recipes will not provide any precise statement on guaranteed
precision for $x>1000$, they will still provide numerically stable and
reasonable results. The power constant is $p=0.933$ in case of the Gaussian
model ($M=G$), and $p=0.5$ in all other cases.
\begin{figure}[tp] %
\begin{center}
\includegraphics[width=0.6\textwidth]{generated/highx.pdf}
\end{center}
\caption{Reference $y^0_M(x)$ and $y^\pi_M(x)$ curves. Dotted lines show a power
  law $y_M(x){\sim}x^{-p}$ with normalisation fixed to coincide with reference
  curves at $x=1000$. The power constant used is $p=0.5$ when $M{\ne}G$ and
  $p=0.933$ when $M=G$.}
\labfig{highx}
\end{figure}

That leaves just the regime of intermediate $x$-values, ranging from the Taylor
threshold at $x=0.1$ or $x=0.02$, and up to $x=1000$. In order to be able to
perform a standard fit of Legendre polynomials to the data, the $x$ values are
first mapped onto the interval from $[-1,1]$ via the transformation:
\begin{align}
  x' = \frac{\sqrt{x}-1}{\sqrt{x}+1}
  \labeqn{xprimetrf}
\end{align}
Here, any power of $x$ would have worked in place of $\sqrt{x}$, but in practice
the subsequent fits are found to more easily align with the data points for this
specific transformation. Likewise, the fits for $y^0_M(x)$ curves also performed
better when the $y$ values are transformed:
\begin{align}
  y' =  \frac{ y + \frac{1}{4}x'(3 - {x'}^2) -\frac{1}{2}}{ ( 1 + x' )^2( 1 - x' ) }
  \labeqn{yprimetrf}
\end{align}
Legendre polynomials are then fitted to the resulting sets of $(x',y')$ points.
The specific form of the $x'$ polynomial in the numerator of \refeqn{yprimetrf}
was chosen to ensure $y'=0$ at $x'=\pm1$ without changing the slope at either of
these points. The denominator was chosen to factor out this boundary condition,
and again the specific powers used in the denominator was chosen to be those
found to make the Legendre polynomials more easily align with the data
points. Although the fits are performed using Legendre polynomials, due to their
many advantages (such as orthogonality and having fitting procedures readily
available in \texttt{NumPy}), the final provided recipes will naturally
first convert the fitted coefficients into coefficients of standard
polynomials. This not only simplifies the final recipes, but also allows for
efficient and robust evaluation of these polynomials via Horner's
method. Additionally, the specific form of the transformation in
\refeqn{yprimetrf} has the advantage that the inverse transformation from $y'$
values back to $y$ values involves polynomials of $x'$ which can simply be
combined with the polynomials from the legendre fits into a single set of
coefficients.

The fits for $y^\Delta_M(x)$ curves are also performed after a transformation of
$x$ values according to \refeqn{xprimetrf}, but do not, however, require a
transformation of $y$ values akin to \refeqn{yprimetrf}. Otherwise the procedure
is similar to the $y^0_M(x)$ case.

That leaves only the choice of order of Legendre polynomial fit to perform, as
well as the number of digits of the coefficients to include in the final
recipes. In short, these choices were carefully investigated and chosen
independently for each particular $y^0_M(x)$ or $y^\Delta_M(x)$ curve, in order
to live up to the provided error guarantees of the particular recipe, while
naturally keeping the recipe as simple as possible. Putting it all together, the
recipe for $y_P(x,\theta)$ is shown in \refalg{yfct_primary}. All other recipes
follow the same general form, with the main difference being the specific
coefficients used, and for reference the other 7 algorithms are included in
\refappendix{recipes}. Additionally, to ease adoption, \texttt{C}, \texttt{C++},
and \texttt{Python} code implementing the recipes as standalone functions are
also made available at \cite{bc2025materials}.
\input{generated/recipe_primary.tex}

\section{Conclusions}
The final precision of both the new standard and luxury recipes from the
present work, as well as the original recipes due to Becker-Coppens, are shown
in \reffig{cmprecipes}. It is clearly seen that while the older recipes are
mostly useful in a more limited range of intermediate $x$ values, the new
recipes live up to the guaranteed error bounds of respectively $10^{-3}$ and
$10^{-6}$. For low $x$ values, the precision is actually even better, which is
not surprising given the usage of Taylor series approximations here.
\begin{figure}[tp]
    \centering
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/cmprecipes_primary.pdf}
        \caption{$M=P$}\labfig{cmprecipes:subfig:P}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/cmprecipes_scndgauss.pdf}
        \caption{$M=G$}\labfig{cmprecipes:subfig:G}
    \end{subfigure}
    \vspace{0.5cm}
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/cmprecipes_scndlorentz.pdf}
        \caption{$M=L$}\labfig{cmprecipes:subfig:L}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{generated/cmprecipes_scndfresnel.pdf}
        \caption{$M=F$}\labfig{cmprecipes:subfig:F}
    \end{subfigure}
    \caption{Precisions of the $y_M(x,\theta)$ values produced by both the
      original BC1974 recipe and the new BC2025 recipes from this work, in both
      standard and luxury versions. The precision is defined as per
      \refeqn{yprecisiondef}, and each recipe contains curves for multiple
      $\theta$ values, with $\theta=0$ and $\theta=\pi/2$ shown with thicker
      lines.}  \labfig{cmprecipes}
\end{figure}

The precise modelling of extinction phenomena in general is an inherently
complex matter, and will by no means be a solved subject as a result of the
present work. But it is nonetheless the hope of the authors that analysis and
modelling codes will adopt these new recipes when applying the Becker-Coppens
models. That way, the research and discussions going forward can focus on the
physics rather than the flawed numerical evaluations plaguing the original work,
which was otherwise extremely detailed and impressive in the subjective opinion
of the authors. In the near future, these updated recipes will be adopted into
\texttt{NCrystal}, and it is the hope and recommendation of the authors that
other refinement or modelling codes supporting Becker-Coppens extinction will
also be able to benefit from the provided updated recipes.

\appendix % if required

\section{Tail integration for the primary extinction model}\labappendix{yptailintegration}
For $M=P$ and $f_P(\eta)$ given by \refeqn{fofeta:P:first}, it is possible to
provide an estimate of the tail of the integral in \refeqn{yintegral}, i.e.\ the
integral over $[a,\infty]$ for some finite and positive $a$. This is achieved by
bounding $f_P(\eta)$ by simpler functions:
\begin{align}
  f_P(\eta) \le f_P^+(\eta) \equiv \frac{\eta^2+\eta+1}{\eta^4}
  \quad\text{and}\quad
  f_P(\eta) \ge {f}_P^-(\eta) \equiv \frac{\eta^2-\eta}{\eta^4}
\end{align}
The function $f^-(\eta)$ is only strictly positive when $\eta>1$, but it is
trivial to impose the restriction $a>2$.  Since $\varphi(\theta,s)$ and
$f_P(\eta)$ decrease monotonically for increasing $s$ and $\eta$ respectively,
for positive arguments, it follows that:
\begin{align}
\int_{a}^{\infty} f_P(\eta)\varphi\left(\theta,xf_P(\eta)\right)\deta &\le
\int_{a}^{\infty} f_P^+(\eta)\varphi\left(\theta,xf^-_P(\eta)\right)\deta \equiv T^+(\theta,x,a) \nonumber\\
\int_{a}^{\infty} f_P(\eta)\varphi\left(\theta,xf_P(\eta)\right)\deta &\ge
\int_{a}^{\infty} f_P^-(\eta)\varphi\left(\theta,xf^+_P(\eta)\right)\deta \equiv T^-(\theta,x,a)
\end{align}
With the help of \texttt{SageMath}, $T^\pm(\theta,x,a)$ can be evaluated
analytically and expressed as a Taylor expansion in inverse powers of $a$:
\begin{align}
  T^+(\theta,x,a) &= \frac{1}{a}  + \frac{1-k}{2a^2 }
  +\left[\frac{1}{3} + \frac{8+4\sin^{5/2}\theta}{25}k^2  \right]\frac{1}{a^3}
  +\mathcal{O}\left(\frac{1}{a^4}\right)\\
  T^-(\theta,x,a) &= \frac{1}{a}  - \frac{1+k}{2a^2 }
  +\frac{8+4\sin^{5/2}\theta}{25}k^2\frac{1}{a^3}
  +\mathcal{O}\left(\frac{1}{a^4}\right)
  \labeqn{tplusminus}
\end{align}
Here $k{\equiv}x/a$ and the requirement on $a$ is therefore tightened again to
$a>\max(x,2)$, in order to ensure $k<1$. Varying both $k$ and $\sin\theta$ over
values in the unit interval and evaluating Taylor coefficients for terms up to
$1/a^{20}$ numerically, it is seen that the maximal absolute numerical value of
coefficients is always within a factor of $2.50$ when compared to the
coefficient of the previous order. Thus, by further restriction to $a>10$, it is
possible to conservatively limit the maximal contributions of the terms of order
$1/a^4$ or higher in \refeqn{tplusminus} to be at most $10/a^4$. With that, one
can then use $(T^+(\theta,x,a)+T^-(\theta,x,a))/2$ as an estimate of the tail
integral value, while $(T^+(\theta,x,a)-T^-(\theta,x,a))/2$ will be a
conservative upper bound of the error on the estimate. In other words, for
$a>\max(10,x)$:
\begin{align}
\int_{a}^{\infty} f_P(\eta)\varphi\left(\theta,xf_P(\eta)\right)\deta \approx
\frac{1}{a}  - \frac{k}{2a^2 }
  +\left[\frac{1}{6} + \frac{8+4\sin^{5/2}\theta}{25}k^2  \right]\frac{1}{a^3}
  \pm
\left(\frac{1}{2a^2 }+\frac{1}{6a^3}+\frac{10}{a^4}\right)
\labeqn{tailintegralpenultimate}
\end{align}
Where the term in the final parenthesis represents an upper bound on the
error. Due to the leading factor of $1/a^2$ in the error term, there is not much
value in keeping the higher order terms. Thus, by a slight additional pessimisation of
the error bound, one arrives at the relatively simple formula for the estimation
of the tail of the integral given in \refeqn{tailintegralP}.

\section{Reference values}\labappendix{yreftables}
For reference, select values of $y_M(x,\theta)$ based on the work described in
\refsec{refeval} are provided in
\reftabrange{yrefvals:P:first}{yrefvals:F:last}.  If more reference values are
needed, the $10^{-6}$ precision of the luxury recipe of \refsec{newrecipes}
should hopefully be adequate for most work. Additionally, a larger set of
reference values are provided in \texttt{JSON} format in \cite{bc2025materials}.

\begin{table}[tp]
\caption{Select reference values of $y_P(x,\theta)$. For compactness, values are
  multiplied by $10^5$ and rounded to the nearest whole number.}
\smallskip
\tiny
\begin{center}
\input{generated/table1_updated.tex}
\end{center}
\labtab{yrefvals:P:first}
\end{table}

\begin{table}[tp]
\caption{Select reference values of $y_G(x,\theta)$. For compactness, values are
  multiplied by $10^5$ and rounded to the nearest whole number.}
\smallskip
\tiny
\begin{center}
\input{generated/table3_updated.tex}
\end{center}
\labtab{yrefvals:G}
\end{table}

\begin{table}[tp]
\caption{Select reference values of $y_L(x,\theta)$. For compactness, values are
  multiplied by $10^5$ and rounded to the nearest whole number.}
\smallskip
\tiny
\begin{center}
\input{generated/table4_updated.tex}
\end{center}
\labtab{yrefvals:L}
\end{table}

\begin{table}[tp]
\caption{Select reference values of $y_F(x,\theta)$. For compactness, values are
  multiplied by $10^5$ and rounded to the nearest whole number.}
\smallskip
\tiny
\begin{center}
\input{generated/tableF_updated.tex}
\end{center}
\labtab{yrefvals:F:last}
\end{table}

\section{The recipes}\labappendix{recipes}
In \refalgrange{yfct_scndgauss}{yfct_scndfresnel} are shown the
standard precision recipes for calculating $y_G(x,\theta)$, $y_L(x,\theta)$,
and $y_F(x,\theta)$, while the recipe for calculating $y_P(x,\theta)$ was
already shown in \refalg{yfct_primary}. Additionally, intended for
high-precision reference work,
\refalgrange{yfct_primary_lux}{yfct_scndfresnel_lux} show the luxury
recipes for calculating the same four functions, but at an increased level of
precision. Note that reference implementations of all the recipes are available
for \texttt{C}, \texttt{C++}, and \texttt{Python} at \cite{bc2025materials}.
\input{generated/recipe_scndgauss.tex}
\input{generated/recipe_scndlorentz.tex}
\input{generated/recipe_scndfresnel.tex}
\input{generated/recipe_primary_lux.tex}
\input{generated/recipe_scndgauss_lux.tex}
\input{generated/recipe_scndlorentz_lux.tex}
\input{generated/recipe_scndfresnel_lux.tex}

\begin{acknowledgements}
  The authors gratefully acknowledge the use of the ESS DMSC computing cluster
  for the numerical evaluations discussed in \refsec{refeval}.
\end{acknowledgements}

\begin{funding}
  This work was supported by the European Spallation Source ERIC.
\end{funding}

\ConflictsOfInterest{The authors declare that there are no conflicts of interest
  related to this publication.}

\DataAvailability{All data, analysis code, and reference implementation of
  recipes of the present work are available at \cite{bc2025materials}.}

\bibliography{iucr} % basename of .bib file

\end{document}
